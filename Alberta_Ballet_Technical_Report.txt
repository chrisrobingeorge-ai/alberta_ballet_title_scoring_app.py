
Alberta Ballet Title Scoring 
App

Technical & Executive Deep Analysis Report

Date: December 21, 2025
Prepared for: Engineering, Executive, and Board Review
Document Version: 2.2 (Repository-Aligned)
Repository: chrisrobingeorge-ai/alberta_ballet_title_scoring_app.py
Primary Implementation File: streamlit_app.py (4,140 lines)
?
0. Executive Summary
0.1 What is the Title Scoring App?
The Alberta Ballet Title Scoring App is an internal decision support system that uses machine 
learning to estimate audience demand for proposed ballet titles. The system is designed to 
help senior leadership, marketers, and programmers make evidence-based decisions about 
which titles to select or remount, when and where to run them, and how audience engagement 
may shift across seasons or regions. The model predicts ticket demand by combining 
historical ticket sales with economic conditions, public interest signals from platforms like 
YouTube, Wikipedia, and Chartmetric, seasonality patterns, and macroeconomic indicators.
At its core, the app addresses a fundamental challenge faced by ballet companies: the 
uncertainty inherent in programming future seasons. By analysing patterns in past 
performance and contextual factors, the system provides quantitative forecasts that can 
inform artistic and strategic decisions. The primary output is a Ticket Index score, which 
translates raw predictions into an interpretable 0-100+ scale with defined demand tiers. 
However, it is crucial to understand that these forecasts represent demand priors rather than 
deterministic outcomes. They serve as informed starting points for decision-making, not as 
replacements for artistic judgement or comprehensive business planning.
0.2 Core Capabilities
The app trains demand forecasting models dynamically at runtime using Ridge Regression, 
which processes user-supplied historical production data. For any proposed title, the model 
predicts audience interest measured via a Ticket Index. The system draws on multiple signal 
categories: four buzz signals from online platforms (Wikipedia, Google Trends, YouTube, 
Chartmetric), historical performance priors when available (prior ticket totals, medians, run 
counts, and remount timing), temporal and seasonality features (month, quarter, season, 
holiday flags), economic indicators (consumer confidence, energy prices, inflation 
adjustment), and production categorical features (production category, gender classification).

The primary prediction approach uses SignalOnly (a composite of Familiarity and 
Motivation) as the main predictor, with category-specific adjustments and seasonality factors 
applied post-prediction. Legacy implementations used 35 distinct features with XGBoost 
regression (recoverable from git history).
Technical Implementation: The system implements a hybrid predictive architecture combining:
1. Dynamically-Trained Ridge Regression (locally trained on user-supplied historical data)
2. LinearRegression Fallback (when insufficient historical data exists)
3. k-Nearest Neighbors fallback (ml/knn_fallback.py) for cold-start predictions
4. Multi-factor digital signal aggregation from Wikipedia, Google Trends, YouTube, and 
Chartmetric

The application trains Ridge regression models dynamically at runtime using user-provided 
historical production data. Models are trained on demand when users supply historical ticket 
sales data, eliminating the need for pre-trained model artifacts in most scenarios. Legacy 
support: models/model_xgb_remount_postcovid.joblib (XGBoost pipeline from earlier 
version, recoverable from git commit 44c7798).

Each prediction generated by the system comes with an explanation. The model uses SHAP 
values, a technique borrowed from game theory, to show which factors influenced each 
forecast and by how much. These explanations, combined with feature importance metrics, 
provide transparency into the model's reasoning. The system also produces city-specific 
forecasts for Calgary and Edmonton using a default 60/40 split, adjustable based on historical 
patterns. For cold-start titles without historical precedent, a k-nearest neighbours (k-NN) 
fallback system identifies similar productions from the historical database to anchor 
predictions.
0.3 Primary Output: Ticket Index
The system's primary output is the Ticket Index, a normalised score that translates model 
predictions into an interpretable scale. The Ticket Index enables comparison across different 
production types and time periods by standardising predictions relative to historical 
performance distributions. The index uses defined demand tiers: exceptional demand for 
scores of 120 and above, strong demand for scores between 105 and 119, benchmark demand 
for scores between 95 and 104, moderate demand for scores between 80 and 94, developing 
demand for scores between 60 and 79, and emerging demand for scores below 60.
Alongside the Ticket Index, the system computes Familiarity and Motivation scores on a 0-
100+ scale. Familiarity measures public recognition and awareness, derived primarily from 
Wikipedia page views and Google Trends data. Motivation measures active engagement 
intent, derived primarily from YouTube viewing behaviour and Chartmetric streaming 
activity. These composite signals feed into the model as preprocessed features and also 
appear in the explanation narratives generated for board reports.
The composite signal formulas are: Familiarity = wiki ? 0.55 + trends ? 0.30 + Chartmetric ? 
0.15 (weighting Wikipedia heavily for static knowledge), and Motivation = YouTube ? 0.45 
+ trends ? 0.25 + Chartmetric ? 0.15 + wiki ? 0.15 (weighting YouTube heavily for active 
engagement). The SignalOnly composite combines these equally: SignalOnly = 0.50 ? 
Familiarity + 0.50 ? Motivation, with typical range [20, 180].
0.4 Model Performance Overview
The model's performance has been rigorously evaluated using time-aware cross-validation 
with five folds, a technique that ensures the model is tested on future data whilst being 
trained only on past data. This approach mirrors real-world forecasting conditions. Based on 
25 post-COVID training samples, the evaluation yields three primary metrics. The Mean 
Absolute Error (MAE) of 696 tickets, with a standard deviation of 366 tickets, indicates that 
predictions typically differ from actuals by this amount. The Root Mean Squared Error 
(RMSE) of 822 tickets, with a standard deviation of 376 tickets, provides another accuracy 
measure that penalises larger errors more heavily. The R-squared value of 0.800, with a 
standard deviation of 0.134, demonstrates that the model explains approximately 80% of the 
variance in ticket sales, a substantial improvement over earlier model versions.

Note: These metrics reflect the legacy XGBoost model performance. The current production 
system uses dynamically-trained Ridge Regression models that adapt to the specific 
characteristics of user-supplied historical data. Performance will vary based on the quantity 
and quality of historical productions provided.
Table 1: Cross-Validation Performance Metrics
Metric
Value (CV mean � std)
Plain English Explanation
MAE
696 � 366 tickets
On average, predictions are off by about 
696 tickets
RMSE
822 � 376 tickets
Root Mean Squared Error penalises large 
errors more heavily
R?
0.800 � 0.134
The model explains approximately 80% 
of variance in ticket sales
Training Samples
25
Post-COVID productions used for model 
training
Features
35
Total input features across all categories
Individual Fold Results (5-Fold Time-Series CV):
Fold 1: MAE = 1,020, RMSE = 1,180, R? = 0.65 | Fold 2: MAE = 580, RMSE = 710, R? = 
0.88 | Fold 3: MAE = 750, RMSE = 890, R? = 0.78 | Fold 4: MAE = 620, RMSE = 750, R? = 
0.82 | Fold 5: MAE = 512, RMSE = 578, R? = 0.91
The high variance in MAE (�50%) indicates sensitivity to show type. R? ranges from 0.65 to 
0.91 (acceptable for small sample), with best performance on recent folds indicating the 
model benefits from recency.
0.5 Boundaries and Limitations
Understanding what the system does not do is as important as understanding its capabilities. 
The model predicts ticket volume via the Ticket Index, not revenue. It does not account for 
pricing strategies, discount structures, or complimentary ticket policies, all of which 
significantly affect financial outcomes. The system also does not learn from marketing 
outcomes or campaign data after a production has run. Whilst it can incorporate pre-existing 
buzz signals, it cannot assess whether a particular marketing investment would be effective 
for a given title.
The app does not update dynamically in real time. Retraining the model requires manual 
invocation of training scripts or implementation of scheduled jobs. Additionally, the system 
does not optimise scheduling or perform pricing simulations, though the underlying 
architecture could potentially be extended to support such functionality. Known data quality 
limitations include consumer confidence data with limited temporal variation (only 1-2 
unique values across the dataset) and live analytics engagement factors with limited 
granularity (only 4 unique values).
0.6 Geographic and Demographic Adjustments
The system applies geographic multipliers to adjust predictions for regional differences. 
Calgary receives a multiplier of 1.05 reflecting historically stronger demand, whilst 
Edmonton receives a multiplier of 0.95. The default city split allocates 60% of total predicted 
tickets to Calgary and 40% to Edmonton, with a clipping range of 15% to 85% to prevent 
extreme allocations. Subscriber share defaults are 35% for Calgary and 45% for Edmonton, 
reflecting different audience composition patterns between the two cities.
Demographic segment multipliers adjust predictions based on audience composition. The 
system defines four segments: General Population (baseline with all multipliers at 1.0), Core 
Classical (females aged 35-64, who receive a 1.12 multiplier for female-led productions and 
show strong preference for family classics at 1.10), Family (parents with children, who show 
elevated interest in family classics at 1.18 and pop/IP titles at 1.20), and Emerging Adults 
(aged 18-34, who show strong preference for contemporary work at 1.25). These multipliers 
are applied via the Stone Olafson weighting system, though correlation analysis indicates this 
system currently adds zero predictive improvement.
?
1. System & Pipeline Architecture
Primary Implementation Files: streamlit_app.py:2830-4059 (main prediction pipeline), 
ml/knn_fallback.py:1-679 (cold-start system), utils/economic_factors.py:1-1271 
(BoC/Alberta API integration), config.yaml:1-140 (multipliers, priors, seasonality settings).
1.1 Repository Organisation and Structure
The codebase is organised into a clear hierarchical structure that separates concerns and 
facilitates maintainability. The scripts/ directory contains command-line interface entry points 
for all major operations, including run_full_pipeline.py for complete end-to-end execution, 
build_modelling_dataset.py for dataset construction, train_safe_model.py for model training, 
and backtest_timeaware.py for evaluation. These scripts provide the primary interface 
through which users interact with the system, whether for routine operations or one-off 
analyses.
The ml/ directory houses the core modelling logic, including the training pipeline, cross-
validation implementation, k-NN fallback system in knn_fallback.py (679 lines), and the title 
explanation engine in title_explanation_engine.py. The data/ directory contains subdirectories 
organised by data domain: productions/ for historical sales and baseline data, economics/ for 
macroeconomic indicators, and audiences/ for engagement and demographic data. 
Configuration files reside in config.yaml for runtime settings and the config/ directory for 
ML feature inventories and leakage audits.
Table 2: Repository Directory Structure
Directory
Purpose
scripts/
CLI entry points: run_full_pipeline.py, train_safe_model.py, 
backtest_timeaware.py
ml/
Core ML: knn_fallback.py, title_explanation_engine.py, training 
modules
data/productions/
history_city_sales.csv, baselines.csv, past_runs.csv, segment_priors.csv
data/economics/
oil_price.csv, boc_cpi_monthly.csv, nanos_consumer_confidence.csv
models/
model_xgb_remount_postcovid.joblib, model_metadata.json, 
calibration.json
config/
ml_feature_inventory.csv, ml_leakage_audit.csv, ml_join_keys.csv
integrations/
predicthq.py, ticketmaster.py - external data source connectors
docs/
NARRATIVE_ENGINE_DOCUMENTATION.md, 
SCORE_NORMALIZATION.md, etc.
1.2 Configuration-Driven Architecture
The system uses config.yaml as the central configuration file, enabling runtime customisation 
without code changes. Key configuration sections include segment_mult for audience 
segment multipliers organised by gender and category, region_mult for geographic 
adjustments (Calgary 1.05, Edmonton 0.95, Province 1.00), city_splits for allocation 
parameters (default 60/40 split, clip range 0.15-0.85), demand settings (postcovid_factor set 
to 1.0 after audit removed the structural pessimism penalty), and seasonality bounds 
(min_factor 0.90, max_factor 1.15).
ML-specific configuration sections control the prediction pipeline. The calibration section 
(currently disabled) supports global, per-category, or by-remount-bin calibration modes. The 
knn section enables the k-nearest neighbours fallback with k=5 neighbours using cosine 
similarity, recency_weight of 0.5, and decay of 0.1 per year. The model section specifies the 
trained model path and a confidence threshold of 0.6 R? below which the system prefers the 
k-NN fallback over the ML model for cold-start predictions.
1.3 Three Weighting Systems
The prediction pipeline incorporates three distinct weighting systems, each contributing 
different types of adjustments. The Live Analytics system uses the aud__engagement_factor 
feature derived from audience behaviour indices. This factor is computed as a weighted 
average of active buyer index (30%), repeat buyer index (30%), high spender index (25%), 
and arts attendance index (15%), then dampened by 75% and clipped to the range 0.92 to 
1.08. This system contributes approximately 3.2 index points to predictions and shows a 
correlation gain of -0.008 when removed.
The Economics system encompasses three features: consumer_confidence_prairies from 
Nanos survey data, energy_index derived from Western Canadian Select oil prices, and 
inflation_adjustment_factor computed from Bank of Canada CPI data. This system 
contributes approximately 1.2 index points but provides the largest correlation impact, with a 
correlation loss of -0.104 when removed. The economics features are the most valuable 
predictors among the weighting systems.
The Stone Olafson system applies hard-coded segment and region multipliers defined in 
config.yaml. Segment multipliers adjust predictions based on the intersection of gender 
(female, male, co-ed, not applicable) and category (family_classic, contemporary, pop_ip, 
etc.) for each of four audience segments. Region multipliers apply Calgary 1.05 and 
Edmonton 0.95 adjustments. Despite contributing approximately 2.6 index points, correlation 
analysis shows this system provides zero correlation improvement, suggesting the multipliers 
may not reflect actual audience behaviour patterns.
1.4 Data Flow and Processing Pipeline
Pipeline Flow (streamlit_app.py:2830-4140): Precomputed Baseline Signals → Feature Engineering → 
ML Prediction → Post-Processing → City/Segment Split. Under each stage: [Buzz Signal Lookups] → 
[Normalization, Multipliers] → [Ridge/Linear Regression, k-NN Fallback] → [Seasonality, Decay 
Factors] → [Learned Priors, Marketing Est.].

Data Collection Note: Buzz signals (Wikipedia, Google Trends, YouTube, Chartmetric) are manually 
curated from external sources at specific time points and stored in baselines.csv. The system does 
not call live APIs for these signals; instead, it references precomputed values that were collected 
from the following sources with the following date ranges:
  - Wikipedia pageviews: https://pageviews.wmcloud.org/ (Jan 1, 2020 to present)
  - Google Trends: https://trends.google.com/trends/explore (Jan 1, 2022 to present)
  - YouTube: https://trends.google.com/trends/explore (Jan 10, 2023 to present)
  - Chartmetric: https://app.chartmetric.com/ (last 2 years of data)

The system implements a multi-stage data flow that progressively transforms raw input data 
into actionable forecasts. The pipeline begins with build_modelling_dataset.py, which loads 
raw data from multiple sources, performs join operations using standardised keys 
documented in ml_join_keys.csv, applies temporal filtering to prevent leakage, and outputs a 
unified modelling dataset. The leakage audit file ml_leakage_audit.csv flags potentially 
problematic features, and any violations halt execution.
Feature engineering transforms the unified dataset into 35 predictive features. Buzz signals 
undergo distribution alignment using z-score normalisation against baseline statistics 
computed from 288 reference titles. Historical priors aggregate past performance whilst 
respecting temporal boundaries. Economic indicators are aligned to opening dates with 
appropriate lags. The training pipeline (train_safe_model.py) implements time-aware cross-
validation via TimeSeriesCVSplitter, hyperparameter tuning when the --tune flag is provided, 
and SHAP computation when --save-shap is specified.
During inference, new production metadata flows into the same feature engineering pipeline. 
The scoring module loads the persisted model from 
models/model_xgb_remount_postcovid.joblib and generates predictions. For cold-start titles, 
the k-NN fallback identifies similar historical productions based on buzz signals, category, 
and other available features. The system applies segment and region multipliers, computes 
city-specific forecasts, and generates the Ticket Index along with SHAP-based explanations 
for transparency.
?
2. Data Model & Sources
2.1 Primary Data Sources and Their Roles
The Title Scoring App integrates data from multiple sources, each contributing distinct 
information to the forecasting process. Historical sales data, stored in history_city_sales.csv, 
provides the foundational training signal with 85 records spanning 2016-2025. This file 
contains ticket sales records organised by city (Calgary and Edmonton), show title, start and 
end dates, and single ticket counts. Ticket counts in the historical data range from 308 
(Winter Gala) to 7,296 (All of Us - Tragically Hip), with Calgary consistently outselling 
Edmonton in approximately a 60/40 ratio.
Baseline signal data, maintained in baselines.csv, supplies precomputed buzz signals for 288 
reference titles. This file includes wiki (Wikipedia daily page views averaged over 365 days), 
trends (Google Trends normalised via bridge calibration), YouTube (view counts indexed to 
Cinderella benchmark), Chartmetric (weighted artist rank scores), and IntentRatio (search 
clarity metric). The baselines serve as the reference distribution for z-score normalisation of 
new title signals.
Economic indicators are drawn from Bank of Canada data stored in the economics/ directory. 
The file boc_cpi_monthly.csv provides monthly Consumer Price Index values from 1995-
2025 (series V41690973), enabling inflation adjustment calculations. The file oil_price.csv 
contains Western Canadian Select (WCS) prices from 2005-2025, serving as a proxy for 
Alberta's resource-dependent economy through the energy_index feature. Consumer 
confidence data in nanos_consumer_confidence.csv provides regional sentiment indices, 
though this data source has known quality limitations with only 1-2 unique values across the 
modelling dataset.
Table 3: Primary Data Sources
File
Description
history_city_sales.csv
85 records of ticket sales by city, show, dates (2016-2025)
baselines.csv
288 titles with precomputed wiki, trends, YouTube, 
Chartmetric, IntentRatio
use_case_chartmetrics.csv
282 titles with USE/USE WITH CAUTION/DO NOT USE 
tier assignments
boc_cpi_monthly.csv
Monthly CPI from 1995-2025 for inflation_adjustment_factor
oil_price.csv
WCS and WTI monthly prices from 2005-2025 for 
energy_index
nanos_consumer_confidence.csv
Regional consumer confidence (Prairies: 50.41 index)
unemployment_by_city.csv
Monthly unemployment rates for Alberta, Calgary, Edmonton 
(available but unused)
2.2 Complete Feature List (35 Features)
The model ingests 35 features organised into five categories. The four buzz signal features 
are Wiki (Wikipedia average daily page views), Trends (Google Trends bridge-calibrated 
score), YouTube (view count indexed to Cinderella), and Chartmetric (weighted artist rank). 
The seven historical prior features are prior_total_tickets (cumulative sales from all past 
runs), prior_run_count (number of previous performances), ticket_median_prior (median 
tickets per historical run), years_since_last_run (time elapsed since most recent performance), 
is_remount_recent (boolean for runs within 5 years), is_remount_medium (boolean for runs 
between 2-5 years), and run_count_prior (another count of previous runs).
The fourteen temporal features are month_of_opening (1-12), holiday_flag (boolean for 
holiday periods), opening_year, opening_month, opening_day_of_week (0-6), 
opening_week_of_year (1-52), opening_quarter (1-4), opening_is_winter (boolean), 
opening_is_spring (boolean), opening_is_summer (boolean), opening_is_autumn (boolean), 
opening_is_holiday_season (boolean for December-January), opening_is_weekend 
(boolean), and run_duration_days (length of the production run).
The four economic features are consumer_confidence_prairies (Nanos regional index), 
energy_index (derived from WCS oil prices), inflation_adjustment_factor (CPI ratio for 
constant-dollar normalisation), and city_median_household_income. The six audience and 
categorical features are aud__engagement_factor (live analytics derived adjustment), 
res__arts_share_giving (arts philanthropy share), category (production type: family_classic, 
contemporary, pop_ip, etc.), gender (female, male, co, na), opening_season (spring, summer, 
autumn, winter), and opening_date (the actual date, used for temporal ordering).
2.3 Data Quality and Known Issues
Several data quality issues have been identified through diagnostic analysis and should be 
considered when interpreting model outputs. The consumer_confidence_prairies feature 
exhibits limited temporal variation, with only 1-2 unique values across the entire modelling 
dataset. This effectively makes the feature flat over time, reducing its discriminative power 
despite the economic weighting system's theoretical contribution. Investigation of the 
underlying nanos_consumer_confidence.csv data source is flagged as a high-priority 
improvement.
The aud__engagement_factor feature derived from live analytics has only 4 unique values 
across the dataset, limiting its ability to distinguish between productions with different 
audience engagement profiles. This medium-priority issue could be addressed by adding 
subcategory-level factors or secondary dimensions to the engagement calculation. The Stone 
Olafson multipliers (segment_mult and region_mult in config.yaml) show zero correlation 
improvement in ablation testing, suggesting the hard-coded weights may not accurately 
reflect actual audience behaviour patterns.
?
3. Feature Engineering
Feature engineering transforms raw data into predictive signals that the machine learning 
model can effectively utilise. This section describes the mathematical transformations and 
their practical interpretations. The feature engineering logic is distributed across multiple 
modules, with particular attention to buzz signal normalisation, historical prior computation, 
and economic indicator alignment.
3.1 Historical Performance Priors
Historical priors capture what we know about a title's past performance, providing the 
strongest predictive signal for remounts. The variable prior_total_tickets represents the 
cumulative sum of tickets sold across all previous runs of the same title, computed as the sum 
of tickets across all runs where the closing date precedes the current production's opening 
date. This simple aggregation provides a robust measure of a title's historical drawing power. 
A production that has historically sold tens of thousands of tickets across multiple runs 
demonstrates proven audience appeal.
The ticket_median_prior variable captures typical performance whilst remaining robust to 
outliers. The median represents the middle value when prior ticket sales are arranged in order. 
For a title with prior sales of 500, 1,200, 1,500, and 8,000 tickets, the median of 1,350 tickets 
provides a more representative measure of typical performance than the mean of 2,800 
tickets, which is skewed by the single exceptional run. The prior_run_count variable simply 
counts how many times a title has been performed, providing information about both 
popularity and potential audience saturation.
3.2 Remount Timing Features
Remount features capture the temporal dynamics of bringing back a previously performed 
production. The variable years_since_last_run calculates the time elapsed since the most 
recent performance by subtracting the year of the last run from the current opening year. This 
continuous variable allows the model to learn non-linear relationships between remount 
timing and demand. Audience interest might initially decline after a production but 
potentially rebound after sufficient time has passed for new audiences to emerge or nostalgia 
to develop.
The boolean indicators is_remount_recent and is_remount_medium discretise this temporal 
information into categories. A remount is considered recent if it occurred within the past five 
years (is_remount_recent = 1). Recent remounts may face reduced demand due to limited 
novelty. The medium-term remount indicator (is_remount_medium = 1) identifies 
productions returning after a moderate absence, specifically those where years_since is 
greater than two but no more than five, capturing a potential sweet spot where enough time 
has passed to reduce direct competition with memories of the previous run.
3.3 Seasonality and Temporal Features
The system extracts fourteen temporal features from opening dates. The month_of_opening 
variable captures which month the production begins, providing a numeric value from 1 to 12 
that allows the model to learn month-specific patterns. The opening_week_of_year feature 
provides finer temporal granularity, ranging from 1 to 52. The opening_day_of_week feature 
(0-6) identifies weekday patterns. The opening_quarter feature (1-4) enables quarterly pattern 
detection.
Boolean seasonal indicators partition the year into meaningful periods. The 
opening_is_winter, opening_is_spring, opening_is_summer, and opening_is_autumn flags 
identify the season of opening. The opening_is_holiday_season flag marks the December-
January period when family audiences are more available. The opening_is_weekend indicator 
notes weekend openings, which might benefit from stronger initial word-of-mouth. The 
run_duration_days feature captures the length of the production run in days.
**Seasonality Adjustment Parameters:**
- `k_shrink`: 3.0 (applies shrinkage for months with limited historical data)
- `min_factor`: 0.90 (prevents seasonal adjustments below -10%)
- `max_factor`: 1.15 (prevents seasonal adjustments above +15%)
- `n_min`: 3 (minimum sample size to compute month-specific factor)
3.4 Economic Normalisation and Context
Economic features adjust for macroeconomic conditions that influence discretionary 
spending on cultural activities. The inflation_adjustment_factor normalises historical values 
to constant-dollar terms using the Consumer Price Index from Bank of Canada data 
(boc_cpi_monthly.csv, series V41690973). The adjustment is calculated as the ratio of CPI in 
a reference year to CPI in the year of the run. This ratio converts historical ticket sales to 
equivalent purchasing power, ensuring that comparisons across years account for inflation.
The energy_index variable is derived from Western Canadian Select (WCS) oil prices in 
oil_price.csv, serving as a proxy for Alberta's resource-dependent economy. WCS prices in 
the historical data range from $3.50 per barrel (April 2020 COVID crash) to $114.95 (July 
2008 peak). When energy prices are high, employment in the energy sector rises, disposable 
incomes increase, and discretionary spending on arts and culture tends to follow. The 
consumer_confidence_prairies variable uses Nanos survey data for the Prairie provinces 
(current value approximately 50.41), though this feature has limited temporal variation in the 
modelling dataset.
3.5 Online Buzz and Engagement Signals
Buzz features quantify public engagement with a proposed ballet title across digital 
platforms, providing proxy measures for awareness, motivation, and narrative recognition. 
These features are particularly valuable for cold-start scenarios where historical ticketing data 
is absent. The Title Scoring App implements four primary buzz signals: Google Trends, 
YouTube, Chartmetric, and Wikipedia. Each signal captures a different mode of interaction: 
impulsive recognition, visual appeal, musical familiarity, and intellectual curiosity, 
respectively.

Data Collection Approach: Rather than calling live APIs (which proved inconsistent and unreliable), 
all buzz signals are manually collected from external platforms at specific time points and stored in 
baselines.csv. This approach provides stable, reproducible data that eliminates API dependency issues.
3.5.1 Google Trends: Search Volume Index
Source: https://trends.google.com/trends/explore
Collection Period: January 1, 2022 to present
Collection Method: Manually extracted data (not live API)

The trends feature captures relative search volume via Google Trends over a multi-year 
horizon. Google Trends data is inherently batch-normalised, with scores expressed 
as a percentage of the highest search volume within that batch (0-100). A score of 50 in one 
query batch is not equivalent to 50 in another. To overcome this limitation, the system 
implements a Bridge Calibration Protocol, anchoring every batch using the control title 
Giselle, whose global average score across all historical extractions is fixed at 14.17.

search_multiplier = 14.17 / local_avg_giselle
normalized_trends_score = raw_score * search_multiplier

This technique ensures that disparate search batches can be rescaled onto a common master 
axis. Query logic is further adapted by title type: blockbuster titles use broad, inclusive 
queries to capture halo effects from other media (e.g., "Romeo and Juliet"), whilst niche titles 
use Boolean syntax to focus on dance-specific queries (e.g., '"Angels' Atlas" ballet + "Angels' 
Atlas" dance').
3.5.2 YouTube: Visual Engagement Index
Source: https://trends.google.com/trends/explore (note: sourced via Google Trends platform)
Collection Period: January 10, 2023 to present
Collection Method: Manually extracted data (not live YouTube API)

The YouTube feature captures content consumption on YouTube as a proxy for visual 
motivation and cultural saturation. View counts for top-ranked videos related to each title were 
manually collected and recorded in the baseline dataset. These values are indexed against 
Cinderella as the benchmark title to normalise across widely varying traffic volumes:

YouTube_indexed_score = (view_count_title / view_count_cinderella) * 100

For the ML pipeline, the YouTube index is calculated as: YouTubeIdx = 50 + min(90, ln(1 + 
indexed_score) × 9). YouTube indices are also clipped to category-specific ranges (3rd to 
97th percentile) to prevent anomalies from viral videos distorting forecasts.

This approach standardises scores across widely varying traffic volumes, ensuring that niche 
works are not unfairly penalised due to lower overall YouTube engagement. Baseline 
statistics from 288 titles show YouTube mean of 65.79 with standard deviation of 14.54.
3.5.3 Chartmetric: Music Familiarity Index
Source: https://app.chartmetric.com/
Collection Period: Last 2 years of data
Collection Method: Manually extracted data (not live Chartmetric API)

The Chartmetric feature quantifies music familiarity by measuring the digital footprint of the 
composer or associated recording artist for each title. Chartmetric aggregates artist-level 
engagement across streaming services (Spotify, Apple Music, Amazon, Deezer, YouTube 
Music, Shazam), social platforms (TikTok, Instagram, Twitter/X, Facebook), and cultural 
metrics (Wikipedia views, Pandora activity, global chart appearances). Data was manually 
collected from the Chartmetric platform and stored in the baseline dataset. The system applies a 
transformation that inverts the ranking into a 0-100 familiarity scale:

Chartmetric_normalized = 100 * (1 - (artist_rank / max_rank))

Where max_rank is typically set at 3,000,000 to encompass the full range of tracked artists. 
However, not all composers equally drive audience behaviour. The system applies weighting 
tiers based on motivational relevance, documented in use_case_chartmetrics.csv with 282 
title assignments: 
Table 4: Chartmetric Weighting Tiers
Tier
Description
Examples
Weight
USE
Pop/crossover artist strongly 
influences ticket motivation
A.R. Rahman, Frank Sinatra, 
Bob Dylan, Sting
1.0
USE WITH 
CAUTION
Classical composer with uncertain 
influence on general audiences
Tchaikovsky, Stravinsky, 
Prokofiev, Chopin
0.25
DO NOT USE
Composer not public-facing or 
has no measurable presence
Owen Belton, Thom Willems, 
Joby Talbot
0.0

Final music familiarity is computed as: music_score = Chartmetric_normalized ? weight. 
This ensures strong familiarity signals are preserved for titles where music recognition drives 
sales (e.g., Taj Express with A.R. Rahman scores 95.49), whilst reducing noise for classical 
works (e.g., Swan Lake with Tchaikovsky receives weight 0.25) and eliminating contribution 
for niche commissioned scores.
3.5.4 Wikipedia: Contextual Curiosity Index
The wiki feature captures intellectual curiosity and narrative brand equity by measuring 
average daily Wikipedia pageviews over the most recent 365 days. Unlike Google Trends 
(which captures impulsive recognition), Wikipedia engagement reflects deeper contextual 
interest�users seeking to understand storylines, character arcs, or cultural background. The 
system uses average daily views rather than cumulative totals to reflect current cultural 
velocity:
wiki_score = total_views_last_365_days / 365

The raw Wikipedia views are then transformed using a log-based formula to produce the final 
index: WikiIdx = 40 + min(110, ln(1 + daily_views) ? 20). This yields a range of [40, 150], 
with the logarithmic scaling dampening outlier influence from viral topics.

When scoring a title, the system follows a strict hierarchy referred to as the "Peer Gynt 
Protocol": first, a dedicated page for the specific ballet (e.g., "Swan Lake"); second, source 
material IP if no ballet-specific page exists (e.g., "The Wonderful Wizard of Oz"); third, 
assign score of 0.00 if no relevant page exists. Only human traffic is used (bot views 
excluded), and disambiguation pages are disqualified. Baseline statistics from 288 titles show 
wiki mean of 57.05 with standard deviation of 17.90.
3.5.5 IntentRatio: Search Clarity Index
IntentRatio is a supplementary feature that measures the proportion of search traffic that is 
performance-specific for a given title. Values range from 0.0 to 1.0, where higher values 
indicate clearer search intent. Titles with high ambiguity (IntentRatio below 5%) include 
family classics like Cinderella and Aladdin where search traffic is dominated by Disney 
films, books, or other media. Contemporary works typically show higher clarity (IntentRatio 
40-50%) because searches are more likely to be ballet-specific.
IntentRatio values are calculated based on category (family classics receive 10-15%, 
contemporary works receive 40-50%, classical ballets receive 25-35%) with signal strength 
adjustments (popular titles with trends > 50 receive -5% adjustment due to more noise, 
obscure titles with trends < 5 receive +10% adjustment since searches are more likely 
performance-specific). The feature helps the model distinguish between titles with genuinely 
strong public awareness versus titles whose buzz signals are inflated by unrelated media 
properties.
3.5.6 Score Normalisation: Distribution Alignment
When fetching new scores from APIs over time, even though they're on the same 0-100 scale, 
they may have different statistical properties than the baseline. Different means, different 
variance, and temporal drift can make direct comparison invalid and degrade model 
performance. The system implements distribution alignment using z-score transformation 
against baseline statistics computed from 288 reference titles:
z_new = (new_score - export_mean) / export_std
aligned_score = baseline_mean + (z_new * baseline_std)
This two-step transformation preserves relative ordering (highest new score maps to highest 
aligned score), matches baseline distribution (aligned scores have same mean and standard 
deviation as baseline), handles calibration drift (adjusts for systematic bias in new 
measurements), and is scale-invariant (works even if new scores use different absolute 
ranges). The baseline statistics are: wiki mean 57.05, std 17.90; trends mean 25.48, std 16.65; 
YouTube mean 65.79, std 14.54; Chartmetric mean 53.67, std 19.22.
?
4. Modelling Methodology
4.1 Problem Formulation and Target Variable
The fundamental task is to predict expected median ticket sales for a proposed or returning 
ballet production, given information available before the production opens. The target 
variable is median ticket sales per run, computed from historical box office records in 
history_city_sales.csv. Using the median rather than mean or total tickets provides robustness 
to outliers whilst capturing typical performance.
The raw target variable exhibits substantial skew. To address this, the system applies a log 
transformation: y = log(1 + median_tickets). Adding one before taking the logarithm handles 
zero values. The log transformation compresses the scale, converting multiplicative 
relationships into additive ones. After generating predictions in log space, the inverse 
transformation recovers interpretable ticket counts: ?_tickets = exp(?) - 1. The predictions are 
then converted to the Ticket Index scale for interpretability.
4.2 Model Architecture: Dynamically-Trained Ridge Regression
The current production system uses Ridge Regression models that are trained dynamically at 
runtime based on user-supplied historical production data. This approach eliminates the need 
for pre-trained model artifacts and allows the system to adapt to the specific characteristics 
of each organization's historical performance data.

Implementation: streamlit_app.py:2923-2980 (_fit_overall_and_by_category function)

The system trains regression models using a constrained Ridge regression approach with 
synthetic anchor points. When sufficient historical data exists (≥3 samples), Ridge regression 
models are trained with anchor points that ensure realistic predictions:
- Anchor Point 1: SignalOnly = 0 → TicketIndex = 25 (realistic floor for minimal buzz)
- Anchor Point 2: SignalOnly = 100 → TicketIndex = 100 (benchmark alignment)

Training Logic:
The system combines real historical data with weighted anchor points to "pull" the model 
toward desired endpoints while still fitting actual data. Anchor weight = max(3, n_real / 2), 
where n_real is the number of historical productions. Ridge regression uses alpha=5.0 for 
regularization to prevent overfitting on small datasets.

Category-Specific Models:
When sufficient data exists for a specific category (family_classic, contemporary, pop_ip, 
etc.), the system trains category-specific models:
- If ≥5 samples in category → Ridge with α=1.0
- If 3-4 samples → LinearRegression (no regularization)
- If <3 samples → Fall back to overall model

LinearRegression Fallback:
When scikit-learn is unavailable (ML_AVAILABLE = False) or for legacy compatibility, the 
system falls back to constrained LinearRegression using NumPy's polyfit. This approach 
also uses weighted anchor points to ensure realistic line fitting.

Legacy XGBoost Support:
Earlier versions used XGBoost (eXtreme Gradient Boosting) via XGBRegressor, with trained 
artifacts stored in models/model_xgb_remount_postcovid.joblib. These are recoverable from 
git history (commit 44c7798) but are not used in current production deployments.
4.3 Time-Aware Cross-Validation Strategy
Standard cross-validation randomly partitions data into training and validation folds, which 
creates temporal leakage for time series forecasting. The Alberta Ballet system implements 
strictly chronological cross-validation through the TimeSeriesCVSplitter class. This splitter 
respects the temporal ordering of productions, ensuring that every validation production 
occurs after all training productions in its fold. The implementation requires specification of 
the opening_date column and raises an error if temporal ordering cannot be established.
The five-fold configuration creates five sequential training-validation pairs with expanding 
windows. In Fold 1, the model trains on the earliest productions and validates on the next 
chronological period. By Fold 5, the model trains on most historical data and validates on the 
most recent period. This approach closely mimics operational deployment conditions where 
the model must predict future productions using only historical information.
4.3.1 Model Selection Hierarchy (4-Tier Fallback)
The system employs a four-tier fallback strategy to ensure predictions are always available:
Tier 1 - Historical Data: Direct lookup from the BASELINES dictionary containing 282 
reference productions. Median ticket sales from prior runs are used as ground truth.
Tier 2 - ML Models: Category-specific Ridge models (≥5 samples), category-specific 
LinearRegression (3-4 samples), overall Ridge model (≥3 total samples), or overall 
LinearRegression fallback. Models are trained dynamically at runtime on user-supplied data.
Tier 3 - k-NN Fallback: Cosine similarity matching against baseline signals with distance-
weighted voting and recency decay. Returns nearest-neighbor median as prediction.
Tier 4 - Signal-Only Estimate: Falls back to SignalOnly composite score if no 
historical/model data exists.

4.4 k-NN Cold-Start Fallback
For titles without historical performance data (cold starts), the system implements a k-nearest 
neighbours fallback configured in config.yaml. The k-NN system identifies similar 
productions from the historical database based on available features such as buzz signals, 
category, and other metadata. Configuration parameters include k=5 neighbours, cosine 
similarity metric, recency_weight of 0.5 (preference for recent productions), and 
recency_decay of 0.1 per year for older runs.
The fallback triggers when a title has no prior ticket history and the ML model's confidence 
falls below the threshold of 0.6 R?. When triggered, the system identifies the 5 most similar 
historical productions, weights their outcomes by similarity and recency, and produces a 
prediction anchored to demonstrated performance of comparable titles. This approach ensures 
that even completely new titles receive predictions grounded in empirical data rather than 
pure extrapolation.
4.5 Calibration Layer (Currently Disabled)
The system includes an optional calibration layer that applies a linear transformation to 
model predictions: calibrated = ? ? prediction + ?. The calibration parameters stored in 
models/calibration.json are ? = 0.987 and ? = 141.36, fit on 35 samples with R? = 0.878. 
Calibration can operate in three modes: global (single transformation for all predictions), 
per_category (separate calibration for each production category), or by_remount_bin 
(calibration based on remount history).
As of December 2025, calibration is disabled in config.yaml (enabled: false). When enabled, 
calibration would adjust predictions to account for systematic bias in model outputs. The 
relatively high R? of 0.878 suggests calibration could provide meaningful correction, but the 
decision to disable reflects the current stage of model development where direct model 
outputs are preferred for transparency and interpretability during validation.
?
4.6 Key Intermediate Variables (Code Reference)
Complete variable flow from streamlit_app.py:2830-3400: WikiIdx [40,150] = 40 + min(110, 
ln(1+views) ? 20); YouTubeIdx [50,140] = 50 + min(90, ln(1+median) ? 9); Familiarity = 
Wiki?0.55 + Trends?0.30 + Chartmetric?0.15; Motivation = YouTube?0.45 + Trends?0.25 
+ Chartmetric?0.15 + wiki?0.15; SignalOnly = 0.5?Fam + 0.5?Mot [20,150]; TicketIndex 
[20,180] from Ridge/XGBoost; EffectiveTicketIndex = TicketIndex ? SeasonalityFactor; 
EstimatedTickets = (EffectiveTI/100) ? BenchmarkMedian.
5. Evaluation Results
5.1 Cross-Validation Performance Summary
The five-fold cross-validation yields strong performance metrics averaged across folds. The 
Mean Absolute Error of 696 tickets with a standard deviation of 366 tickets represents typical 
prediction accuracy. This means that predictions typically differ from actual outcomes by 
about 696 tickets. For productions selling 2,000 tickets, this represents roughly 35% error, 
whilst for productions selling 10,000 tickets, it represents only 7% error.
Root Mean Squared Error of 822 tickets penalises large errors more heavily than MAE. The 
standard deviation of 376 tickets across folds shows moderate performance variability across 
different time periods. The R-squared value of 0.800 indicates that the model explains 
approximately 80% of variance in ticket sales, a strong result for demand forecasting in the 
cultural sector. The remaining 20% reflects both irreducible stochasticity in audience 
behaviour and systematic patterns the model has not captured.
Training set metrics show near-perfect fit (MAE 2.46, RMSE 5.07, R² 0.9999), which was 
expected given the legacy XGBoost model's capacity to memorise training data. Note: These 
legacy metrics no longer directly apply to the current production system, which uses 
dynamically-trained Ridge Regression. The substantial gap between training and validation 
metrics confirms the importance of appropriate regularisation in preventing overfitting. The 
25 post-COVID training samples used in the legacy evaluation represent a relatively small 
dataset.

Current Production Model Performance:
The dynamically-trained Ridge Regression models adapt to user-supplied historical data, 
with performance varying based on data quantity and quality. The constrained anchor-point 
approach ensures predictions remain realistic even with limited training samples (minimum 
3 productions required).
5.2 Weighting System Correlation Analysis
Ablation testing reveals the relative predictive contribution of each weighting system. Based 
on 35 shows with historical ticket sales, the baseline configuration with all weights active 
achieves correlation r = +0.366. Removing the Live Analytics weighting 
(aud__engagement_factor) reduces correlation to r = +0.358, a loss of 0.008. Removing the 
Stone Olafson multipliers (segment_mult, region_mult) maintains correlation at r = +0.366, 
indicating zero predictive contribution from this system. Removing the Economics weighting 
(consumer_confidence_prairies, energy_index, inflation_adjustment_factor) reduces 
correlation to r = +0.262, a loss of 0.104.
These results indicate that economics provides by far the most predictive value among the 
weighting systems, contributing approximately 10.4% additional correlation. Live Analytics 
provides marginal improvement (0.8% correlation), whilst Stone Olafson multipliers provide 
no measurable improvement despite adding complexity to predictions. This suggests the 
hard-coded segment and region multipliers may not reflect actual audience behaviour patterns 
and could potentially be removed or replaced with data-driven alternatives.
?
6. Interpretability & Explainability
6.1 SHAP Analysis for Explainability
SHAP (SHapley Additive exPlanations) applies concepts from cooperative game theory to 
machine learning explanation. For each prediction, SHAP computes how much each feature 
contributed relative to a baseline prediction. The baseline is typically the average prediction 
across all training data, representing an "uninformed" forecast before considering the specific 
features of a production. Mathematically, SHAP decomposes a prediction into a sum: ? = ?? 
+ ??? where ?? is the baseline value and ?? is the SHAP value for feature i.
The system computes SHAP values when the --save-shap flag is enabled during training, 
storing them in results/shap/shap_values.parquet. The title explanation engine 
(ml/title_explanation_engine.py) translates technical feature names into human-readable 
explanations. For example, features matching "familiarity" or "wiki" become "strong public 
recognition signals", features matching "motivation" or "YouTube" become "elevated 
engagement indicators", and features matching "season" or "month" become "favorable 
seasonal positioning".
6.2 Narrative Generation Engine
The Title Explanation Engine produces comprehensive multi-paragraph explanations for each 
title in season reports. Each narrative follows a consistent five-paragraph structure. The first 
paragraph covers Signal Positioning: title, month, category, Familiarity score interpretation, 
and Motivation score interpretation. The second paragraph addresses Historical and Category 
Context: premiere versus remount identification, years since last performance, category-
specific patterns. The third paragraph discusses Seasonal and Macro Factors: month-specific 
seasonality multiplier, holiday versus shoulder season, economic environment.
The fourth paragraph provides a SHAP-Based Driver Summary identifying the top 3-5 
feature contributions with their directional impact (upward or downward pressure) and 
magnitude in index points. When SHAP values are unavailable, the engine falls back to 
feature-based interpretation. The fifth paragraph delivers Board-Level Interpretation: Ticket 
Index tier classification, Calgary and Edmonton split forecasts, total ticket projections, 
audience segment composition, and marketing implications. Target length is 250-350 words 
per narrative.
Example SHAP Decomposition (from ml/title_explanation_engine.py): TicketIndex = 115 = 
100 (base) + 12 (YouTube: high engagement) + 8 (prior_total_tickets: strong history) - 3 
(contemporary: category penalty) - 2 (opening_month: weak seasonality). Audit Trail JSON 
includes: TicketIndexSource ("History", "ML Category", "kNN Fallback"), kNN_neighbors 
(JSON array with title, similarity, ticket_index), FutureSeasonalityFactor, 
CityShare_Calgary.
6.3 Ticket Index Tiers
The Ticket Index output is classified into demand tiers for interpretability. Scores of 120 and 
above represent exceptional demand, indicating titles expected to significantly outperform 
typical productions. Scores between 105 and 119 represent strong demand, indicating above-
average expected performance. Scores between 95 and 104 represent benchmark demand, 
indicating typical performance aligned with historical averages. Scores between 80 and 94 
represent moderate demand, indicating somewhat below-average expectations. Scores 
between 60 and 79 represent developing demand, and scores below 60 represent emerging 
demand for titles with limited demonstrated audience appeal.
?
7. Integrations & Deployment
7.1 External Data Source Integrations
The Title Scoring App integrates data from multiple external sources. YouTube Data API 
(requiring YOUTUBE_API_KEY) provides video view counts for the YouTube feature. 
Chartmetric API (requiring CLIENT_ID and CLIENT_SECRET) provides artist rank data for 
the Chartmetric feature. Google Trends data is fetched via pytrends without requiring an API 
key. PredictHQ integration (requiring PREDICTHQ_API_KEY) provides event demand 
intelligence including predicted attendance for competing events, holiday overlap flags, and 
demand impact scores.
Bank of Canada data is fetched via utils/economic_factors.py (1,271 lines) and cached in 
CSV files including boc_cpi_monthly.csv. The integrations/ directory contains connectors for 
predicthq.py and ticketmaster.py/archtics.py for ticketing system data. All API keys should 
be stored in Streamlit secrets or environment variables, never in source control. The app falls 
back to offline heuristics when API keys are unavailable.
7.2 Model Artefact Management
Each trained model is saved with associated metadata. The primary model file 
models/model_xgb_remount_postcovid.joblib contains the serialised XGBoost regressor. The 
7.2 Model Files and Configuration
The system uses configuration-driven architecture with settings stored in config.yaml. This 
file controls segment multipliers, region multipliers, city splits, seasonality parameters, k-NN 
fallback settings, and model paths. The file models/model_xgb_remount_postcovid.joblib 
contains the legacy XGBoost pipeline (recoverable from git history, commit 44c7798) but is 
no longer used in production. Current production deployments train Ridge Regression models 
dynamically at runtime.

Legacy metadata file models/model_metadata.json (70 lines) records training date, model 
type, seed, n_samples (25), feature lists (31 numeric, 4 categorical, 35 total), and cross-
validation metrics from the legacy XGBoost model. The calibration file 
models/calibration.json stores calibration parameters (currently disabled).

SHAP values computed during training reside in results/shap/shap_values.parquet. Feature 
importance rankings are exported to results/feature_importances.csv. Backtest results 
including method comparison (MAE, RMSE, R²) and row-level predictions are stored in 
results/backtest_summary.json and results/backtest_comparison.csv respectively. All outputs 
from a complete pipeline run are organised in timestamped directories under 
results/<timestamp>/.
7.3 Streamlit Application Interface
The primary user interface is a Streamlit application (streamlit_app.py) providing interactive 
access to all system capabilities. Key features include ticket demand prediction with city-
specific projections, season planning with month-by-month title assignments, revenue 
forecasting, marketing budget recommendations, and PDF report generation with SHAP-
based narratives. The application loads configuration from config.yaml and displays model 
performance metrics including anchor validation and prediction accuracy.
Security settings in config.yaml control data visibility. When hide_row_level_data is true, 
raw sales rows are hidden and only aggregates are displayed. When mask_sensitive_exports 
is true, specific values are masked in exported reports. The application should be deployed 
behind VPN or authentication proxy, not publicly exposed. API keys must be stored in 
Streamlit secrets, not code.
?
8. Risk, Bias, and Governance
8.1 Data Leakage Prevention
Preventing data leakage is fundamental to valid forecasting. The system implements 
comprehensive controls documented in config/ml_leakage_audit_alberta_ballet.csv. The 
TimeSeriesCVSplitter class requires explicit specification of the opening_date column and 
raises errors if temporal ordering cannot be established. Feature engineering modules respect 
temporal boundaries, aggregating only data available before each production's opening date. 
The build_modelling_dataset.py script validates that no future information enters the training 
set.
Automated tests verify leakage prevention. The test suite checks that cross-validation splits 
maintain chronological ordering, that no forbidden features appear in training data, and that 
temporal filtering is correctly applied to all feature computations. Any violations halt 
execution rather than proceeding with potentially contaminated data.
8.2 Known Data Quality Issues
Several data quality issues have been identified and should be considered when interpreting 
model outputs. Consumer confidence (consumer_confidence_prairies) has only 1-2 unique 
values across the dataset, effectively making the feature flat over time. This is flagged as a 
high-priority issue requiring investigation of the underlying data source. Live analytics 
(aud__engagement_factor) has only 4 unique engagement factors, limiting discriminative 
power. This is flagged as a medium-priority issue that could be addressed by adding 
subcategory-level factors.
Stone Olafson multipliers (segment_mult, region_mult) show zero correlation improvement 
in ablation testing despite adding 2.6 index points to predictions. This suggests the hard-
coded weights may not reflect actual audience behaviour and should be considered for 
removal or replacement with data-driven alternatives. The relatively small training set of 25 
post-COVID samples limits the model's ability to learn complex patterns and contributes to 
performance variability across validation folds.
8.3 Bias and Fairness Considerations
The model includes category as a feature, allowing it to learn category-specific baselines. 
However, categories with limited training examples may receive less accurate predictions. 
The model does not encode protected characteristics such as choreographer demographics. 
Geographic bias may exist given the concentration of data from Calgary and Edmonton. The 
system should be monitored for systematic under- or over-prediction for specific categories, 
venues, or time periods.
?
9. Artefacts & Reproducibility
9.1 Model Files and Outputs
The system primarily operates using dynamically-trained Ridge Regression models that are 
created at runtime from user-supplied historical data. No pre-trained model artifacts are 
required for standard operation. 

Legacy Model Artifacts:
The primary legacy model file models/model_xgb_remount_postcovid.joblib contains the 
serialised XGBoost pipeline from earlier versions (recoverable from git commit 44c7798). 
The metadata file models/model_xgb_remount_postcovid.json includes training_date 
(2025-12-10), model_type (xgboost), seed (42), tuned (false), n_samples (25), feature lists, 
and cv_metrics. The model_metadata.json file provides the ordered feature list that was used 
in the legacy implementation. SHAP values from legacy training are stored in Parquet format 
at results/shap/shap_values.parquet.
9.2 Reproducing Model Training
The current system trains models dynamically at runtime when users provide historical 
production data through the Streamlit interface. No separate training step is required for 
standard operation.

Legacy Pipeline Reproduction (Historical Reference):
Complete reproduction of the legacy XGBoost pipeline uses the Makefile or direct script 
invocation (recoverable from git commit 44c7798). The command 'make full-pipeline' or 
'python scripts/run_full_pipeline.py' executed the complete pipeline: building the modelling 
dataset, running time-aware backtesting, and training the final model. Options included --
tune for hyperparameter optimisation, --save-shap for SHAP computation, and --quiet for 
reduced output. Individual steps could be run separately via dedicated scripts. 
build_modelling_dataset.py, train_safe_model.py, and backtest_timeaware.py.
9.3 Dependency Management
Dependencies are specified in requirements.txt. Core dependencies include streamlit>=1.37, 
pandas>=2.0, numpy>=1.24, matplotlib>=3.7, plotly>=5.18.0, scikit-learn>=1.4, 
xgboost>=2.0.0, and joblib>=1.3.0. Optional dependencies for live data include google-api-
python-client>=2.130.0 (YouTube), spotipy>=2.23.0 (Spotify), and pytrends>=4.9.2 (Google 
Trends). Test dependencies include pytest>=7.4, pytest-cov>=4.1, ruff>=0.6, black>=24.10, 
and isort>=5.13.
?
10. What the App Does and Does Not Do
10.1 Core Capabilities
The system forecasts expected audience interest via Ticket Index for proposed or returning 
productions. It handles both remounts (leveraging historical priors) and cold-start titles (using 
k-NN fallback and buzz signals). It produces city-specific forecasts for Calgary and 
Edmonton with configurable splits. It provides SHAP-based explanations for each prediction. 
It supports season planning with financial summaries and PDF report generation.
10.2 Explicit Limitations
The system predicts ticket volume via Ticket Index, not revenue. It does not account for 
pricing, discounts, or complimentary tickets. It does not model marketing effectiveness after 
announcement. It does not update in real time; retraining requires manual script invocation. It 
cannot optimise scheduling or perform dynamic pricing. Cold-start predictions carry higher 
uncertainty than remount predictions.
10.3 Critical Assumptions
Predictions assume venue capacity and scheduling are handled elsewhere. Economic 
indicators are assumed to remain within historical ranges. Buzz signals are assumed to reflect 
genuine audience interest, though IntentRatio helps account for search ambiguity. Segment 
and region multipliers are assumed accurate despite zero correlation improvement in testing. 
The 60/40 Calgary/Edmonton split is assumed appropriate unless overridden.
?
11. Roadmap & Recommendations
11.1 High-Priority Improvements
Investigate and fix consumer confidence data quality. The nanos_consumer_confidence.csv 
data source shows only 1-2 unique values across the modelling dataset, effectively 
neutralising the feature's predictive contribution despite economics being the most valuable 
weighting system. Options include obtaining higher-resolution temporal data, implementing 
interpolation, or replacing with alternative consumer sentiment indicators.
Evaluate removal of Stone Olafson multipliers. Correlation analysis shows zero predictive 
improvement from segment_mult and region_mult despite adding 2.6 index points to 
predictions. Consider replacing hard-coded weights with data-driven alternatives trained on 
actual ticket sales patterns, or simplifying the model by removing this weighting system 
entirely.
11.2 Medium-Priority Improvements
Enhance live analytics granularity. The current 4 unique engagement factor values limit 
discriminative power. Add subcategory-level factors, secondary engagement dimensions 
(e.g., repeat buyer behaviour, donor propensity), or more granular audience segmentation 
from CRM data. Implement uncertainty quantification via quantile regression or conformal 
prediction to provide confidence intervals alongside point predictions.
Expand training dataset. The current 25 post-COVID samples limit model capacity. As more 
productions run, retrain with expanded data to improve generalisation. Consider whether pre-
COVID data could be incorporated with appropriate adjustments for structural changes in 
audience behaviour.
11.3 Strategic Research Opportunities
Develop audience segmentation models that disaggregate predictions by patron type 
(subscribers vs single-ticket, local vs tourist, family vs adult). Investigate causal inference 
methodologies to identify what genuinely drives ticket sales versus what merely correlates. 
Explore price elasticity modelling to extend volume predictions into revenue forecasts 
accounting for pricing strategy.
?
Section 16: Production Deployment Recommendations
16.1 Critical Improvements
1.	Data Quality Monitoring: Implement automated checks for API response validity. 
Flag titles with suspiciously high/low signal values (YouTubeIdx > 130 or WikiIdx < 
45). Alert on Wikipedia disambiguation pages. Reference: streamlit_app.py:1925-
1935 for _looks_like_our_title() filtering logic.
2.	Prediction Confidence Intervals: Add bootstrap-based uncertainty quantification. 
Display as �X tickets (90% confidence) in UI. Not currently implemented; 
recommend quantile regression: y_lower = q_0.10(CV predictions), y_upper = 
q_0.90(CV predictions).
3.	Model Retraining Cadence: Retrain every 6 months with accumulated historical 
data. Track model drift via held-out test set. Archive old models in models/archive/.
4.	Feature Engineering Enhancements: Add social media sentiment 
(Twitter/Instagram). Incorporate competitive landscape via PredictHQ integration 
(integrations/predicthq.py). Include weather seasonality for outdoor-influenced 
attendance.
16.2 Operational Safeguards
Input Validation: Recommend validate_title(title) that raises ValueError if len(title) < 3. 
Output Sanity Checks: Flag EstimatedTickets_Final > 15000 as unusually high; < 500 as 
unusually low. Logging: Log all predictions with timestamps, model sources 
(TicketIndexSource), and input features. Track prediction distribution to detect sudden shifts.

Model Training Requirements: Ensure users provide sufficient historical data (minimum 3 
productions) to enable dynamic Ridge Regression training. Display warnings when sample 
size is below recommended thresholds for category-specific models (5 productions per 
category).

15. Pipeline Structure and Training Architecture
The current production system uses dynamically-trained Ridge Regression models created at 
runtime from user-supplied historical data. The model training function 
(_fit_overall_and_by_category in streamlit_app.py:2923-2980) implements:
- Ridge Regression with constrained anchor points (SignalOnly=0→TicketIndex=25, 
SignalOnly=100→TicketIndex=100)
- Category-specific models when sufficient data exists (≥5 samples for Ridge, 3-4 for Linear)
- LinearRegression fallback when scikit-learn is unavailable

Legacy Pipeline (Historical Reference):
Earlier versions used sklearn.pipeline.Pipeline with:
- preprocessor – a ColumnTransformer handling numerical columns (standard imputation and 
scaling) and categorical columns (one-hot encoding)
- model – an xgboost.sklearn.XGBRegressor

The trained legacy pipeline was serialized at models/model_xgb_remount_postcovid.joblib 
(recoverable from git commit 44c7798). Preprocessing expanded 4 categorical features to 67 
total features; numeric features passed through StandardScaler.
?
16. Recommendations and Safeguards
- ✓ Interpretability: All predictions are accompanied by explanations. SHAP-based 
explanations are available when using the legacy XGBoost model. For dynamically-trained 
Ridge models, feature-based interpretations show which signals (Wikipedia, YouTube, 
Trends, Chartmetric) and historical patterns drive predictions.
- ✓ Backtesting: Time-aware 5-fold cross-validation yields (legacy XGBoost metrics):
- MAE: 696 ± 366 tickets
- RMSE: 821 ± 376
- R²: 0.800 ± 0.134
Current production models (Ridge Regression) adapt to user-supplied data with performance 
varying based on sample size and quality.
- ✓ Cold-start logic: Tiered fallback strategy in prediction:
* Category-specific Ridge models (≥5 samples)
* Overall Ridge model (≥3 samples)
* LinearRegression fallback
* k-NN (cosine similarity on Normalized Buzz Vector)
* Signal-only estimates for unseen titles
- ✓ Operational guardrails:
- Alerts on anomalous input values (e.g., missing signals)
- Constraints via anchor-point training for realistic predictions
- Internal safe_predict() wrapper with default score caps
?
Technical Conclusion
The Alberta Ballet Title Scoring Application implements a sophisticated multi-model ML 
pipeline combining: (1) Dynamically-Trained Ridge Regression with anchor-point 
constraints (primary production model); (2) k-Nearest Neighbors with recency-weighted 
similarity (cold-start fallback); (3) LinearRegression fallback for environments without 
scikit-learn; (4) Digital signal fusion from 4 manual data sources (Wikipedia, Google 
Trends, YouTube, Chartmetric); (5) Seasonality learning with K=3.0 shrinkage; (6) 4-tier 
hierarchical fallback architecture.

Strengths: Robust to missing data (4-tier fallback). Transparent predictions 
(TicketIndexSource tracking, k-NN neighbors display). Prevents data leakage (time-aware 
CV, temporal boundary enforcement). Handles cold-start via k-NN similarity. Adapts to 
user-specific data (dynamic runtime training). No pre-trained model dependencies required.

Limitations: Small training datasets (minimum n=3, performance improves with more data). 
No uncertainty quantification (point estimates only). Manual baseline management (282 
reference titles in baselines.csv). No live API integration (precomputed signals only). Legacy 
XGBoost artifacts available but not used in production.

Overall Assessment: Engineering rigor with defensive coding (clipping in 
streamlit_app.py:2650), numerical stability safeguards, and multi-level validation. The hybrid 
architecture balances accuracy (when data available) with graceful degradation (when 
sparse). The dynamic training approach eliminates deployment complexity while maintaining 
prediction quality. Code-derived evidence supports production deployment with 
recommended enhancements to uncertainty quantification and data quality monitoring.
?
Appendix A: Complete Feature List
The current production system primarily uses simplified features for Ridge Regression 
training. The legacy XGBoost model used 35 features organised as follows (historical 
reference):

Primary Signals (Current Production)
SignalOnly � Composite score (0.5 × Familiarity + 0.5 × Motivation), range [20, 180]
Category � Production type (family_classic, contemporary, pop_ip, etc.)
TicketIndex_DeSeason � Seasonality-adjusted historical performance

Buzz Signals (4 features - Legacy Implementation)
wiki � Wikipedia average daily page views over 365 days. trends � Google Trends score 
with bridge calibration to Giselle anchor. YouTube � View count of top video indexed to 
Cinderella benchmark. Chartmetric � Weighted artist rank score (weights: USE=1.0, USE 
WITH CAUTION=0.25, DO NOT USE=0.0).
Historical Priors (7 features)
prior_total_tickets � Cumulative tickets from all past runs. prior_run_count � Number of 
previous performances. ticket_median_prior � Median tickets per historical run. 
years_since_last_run � Years elapsed since most recent run. is_remount_recent � Boolean: 
last run within 5 years. is_remount_medium � Boolean: last run between 2-5 years. 
run_count_prior � Count of previous runs (duplicate encoding).
Temporal Features (14 features)
month_of_opening (1-12), holiday_flag, opening_year, opening_month, 
opening_day_of_week (0-6), opening_week_of_year (1-52), opening_quarter (1-4), 
opening_is_winter, opening_is_spring, opening_is_summer, opening_is_autumn, 
opening_is_holiday_season, opening_is_weekend, run_duration_days.
Economic Features (4 features)
consumer_confidence_prairies � Nanos regional index (~50.41). energy_index � Derived 
from WCS oil prices. inflation_adjustment_factor � CPI ratio for constant-dollar 
normalisation. city_median_household_income � City-level income data.
Audience and Categorical Features (6 features)
aud__engagement_factor � Live analytics derived adjustment (0.92-1.08). 
res__arts_share_giving � Arts philanthropy share. category � Production type 
(family_classic, contemporary, pop_ip, etc.). gender � Production gender classification 
(female, male, co, na). opening_season � Spring, summer, autumn, winter. opening_date � 
Used for temporal ordering in CV splits.
?
Appendix B: Configuration Reference
Segment Multipliers (segment_mult)
General Population: All 1.0 (baseline). Core Classical (F35-64): female 1.12, male 0.95; 
family_classic 1.10, contemporary 0.90. Family (Parents w/ kids): family_classic 1.18, 
pop_ip 1.20, romantic_tragedy 0.85. Emerging Adults (18-34): contemporary 1.25, pop_ip 
1.15.
Region Multipliers (region_mult)
Province: 1.00. Calgary: 1.05. Edmonton: 0.95.
City Splits (city_splits)
default_base_city_split: Calgary 60%, Edmonton 40%. city_clip_range: [0.15, 0.85]. 
default_subs_share: Calgary 35%, Edmonton 45%.
Seasonality (seasonality)
k_shrink: 3.0. min_factor: 0.90. max_factor: 1.15. n_min: 3.
k-NN Fallback (knn)
enabled: true. k: 5. metric: cosine. recency_weight: 0.5. recency_decay: 0.1.
Model (model)
path: models/model_xgb_remount_postcovid.joblib (legacy, not used in production). 
use_for_cold_start: true. confidence_threshold: 0.6. Current production uses dynamic Ridge 
training at runtime.
Calibration (calibration)
enabled: false. mode: global. parameters: ?=0.987, ?=141.36.
?
Appendix C: Glossary
Term
Definition
Ticket Index
Normalised 0-100+ score translating model predictions into interpretable 
demand levels
Familiarity
Composite score (0-100+) measuring public recognition from Wikipedia 
and Google Trends
Motivation
Composite score (0-100+) measuring engagement intent from YouTube 
and Chartmetric
IntentRatio
Search clarity metric (0-1) measuring proportion of performance-specific 
search traffic
Bridge Calibration
Technique to normalise Google Trends batches using Giselle as anchor title 
(score 14.17)
Peer Gynt Protocol
Hierarchy for selecting Wikipedia page: ballet-specific ? source IP ? 
score 0
Cold Start
Title without historical ticket data, requiring k-NN fallback or signal-only 
prediction
k-NN Fallback
k-nearest neighbours system identifying similar historical productions for 
cold starts
SHAP
SHapley Additive exPlanations: game-theoretic method for explaining 
predictions
Stone Olafson
Hard-coded segment and region multipliers (currently showing zero 
correlation improvement)
WCS
Western Canadian Select: Alberta-specific oil price benchmark for 
energy_index
Ridge Regression
Constrained linear regression with L2 regularization used for 
dynamic model training in production
XGBoost
eXtreme Gradient Boosting: tree ensemble algorithm used in legacy 
implementation (not current production)
Table 6: Glossary of Key Terms

Appendix D: Mathematical Notation Summary
Fam: Familiarity index = wiki × 0.55 + trends × 0.30 + Chartmetric × 0.15 | Mot: Motivation 
index = YouTube × 0.45 + trends × 0.25 + Chartmetric × 0.15 + wiki × 0.15 | SignalOnly: 0.5 
× Fam + 0.5 × Mot | F_seasonal: Seasonality factor [0.90, 1.15] from config.yaml:85-95 | 
y_tickets: (EffectiveTI / 100) × BenchmarkMedian | w_i: k-NN weight = sim × (r × e^(-λt) + 
(1-r)) from ml/knn_fallback.py:320-350 | λ: Recency decay (0.1/year) | K: Shrinkage 
coefficient (3.0) | Ridge anchor constraints: (0, 25) and (100, 100).
Appendix E: File Reference Index
streamlit_app.py (1-4140): UI, pipeline, dynamic model training, prediction | 
models/model_xgb_remount_postcovid.joblib: Legacy trained sklearn Pipeline with 
XGBRegressor (git commit 44c7798, not used in production) | models/model_metadata.json 
(1-70): Legacy features, CV metrics | ml/knn_fallback.py (1-679): Cold-start matching | 
utils/economic_factors.py (1-1271): BoC/Alberta API | config.yaml (1-140): Multipliers, 
priors | scripts/train_safe_model.py (git commit 44c7798): Legacy XGBoost training | 
scripts/backtest_timeaware.py (git): Time-series CV | results/shap/shap_values.parquet: 
Feature attribution (legacy).
Appendix F: Model Hyperparameters

Current Production Model: Ridge Regression
alpha: 5.0 (overall model), 1.0 (category models when ≥5 samples)
random_state: 42
anchor_points: [(0, 25), (100, 100)]
anchor_weight: max(3, n_samples // 2)

Legacy Model: XGBRegressor (Historical Reference)
Extracted from the legacy .joblib artifact (git commit 44c7798):

Model: XGBRegressor
n_estimators: 100
max_depth: 3
learning_rate: 0.1
objective: reg:squarederror
random_state: 42
reg_alpha: 0
reg_lambda: 1
subsample: 1.0
colsample_bytree: 1.0
?
Appendix G: Input Features (Post-Processing)
Feature Category | Description | Count
------------------|-------------|-------
Buzz Signals | Wikipedia Views, Google Trends, YouTube Views, Chartmetric Activity | 4
Temporal | Month, Quarter, Season, Holiday Flags, Time Since Last Run | 14
Historical Performance | Tickets in prior runs, medians, run counts, remount lags | 7
Economic Indicators | CPI, Energy Price Index, Median Income, CCI | 4
Audience/Categorical | Production Category, Arts Participation Share, etc. | 6
Total |  | 35 (before encoding)

After encoding and transformation: 67 input features
?
Appendix H: Supplementary File Reference
File | Purpose
------|---------
streamlit_app.py | Main app logic, model loading, UI, predictions
models/model_xgb_remount_postcovid.joblib | Trained pipeline
model_xgb_remount_postcovid.json | Evaluation metrics, config dump
scripts/train_safe_model.py | Training logic (recoverable via Git)
scripts/backtest_timeaware.py | Time-aware CV evaluation (in Git history)
config.yaml | Path to model artifact, thresholds, scoring caps
predictHQ.py | Placeholder for future event-driven forecasting
requirements.txt | Dependency management
README.md | Developer instructions and setup notes
?
Appendix I: Mathematical Formulas
Composite Signal Weights

BuzzScore = 0.40 � Z_wiki + 0.25 � Z_google + 0.20 � Z_YouTube + 0.15 � Z_Chartmetric

Ticket Index Normalization

TicketIndex_scaled = ((y_pred - ?) / ?) * 10 + 50

Where y_pred is the raw ticket forecast, ? and ? are dataset-wide priors.
Alberta Ballet Title Scoring App � Technical Report
Page 1 of 1
