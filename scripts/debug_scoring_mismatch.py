"""
Debug scoring mismatch between title_scoring_helper.py and baselines.csv.

This script compares the scores generated by the title_scoring_helper logic
against the expected scores in baselines.csv to identify discrepancies.
"""

import sys
from pathlib import Path

# Add repo root to path
_REPO_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(_REPO_ROOT))

import pandas as pd
import numpy as np


def load_baselines():
    """Load the baseline scores from CSV."""
    csv_path = _REPO_ROOT / "data" / "productions" / "baselines.csv"
    return pd.read_csv(csv_path)


def normalize_0_100(values):
    """
    Rescale a list of values to 0â€“100.
    This is the exact function from title_scoring_helper.py
    """
    if not values:
        return []
    v = [0.0 if (val is None or pd.isna(val)) else float(val) for val in values]
    v_min = min(v)
    v_max = max(v)
    if v_max <= v_min:
        return [50.0 for _ in v]
    return [100.0 * (x - v_min) / (v_max - v_min) for x in v]


def analyze_normalization_impact(df):
    """
    Analyze how the normalization function affects scores.
    
    The key issue: title_scoring_helper.py normalizes scores RELATIVE to the
    batch of titles being scored, not globally across all titles.
    """
    print("\n" + "="*80)
    print("NORMALIZATION ANALYSIS")
    print("="*80)
    
    for signal in ['wiki', 'trends', 'youtube', 'chartmetric']:
        values = df[signal].dropna().tolist()
        
        print(f"\n{signal.upper()}:")
        print(f"  Raw range in baselines.csv: [{min(values):.2f}, {max(values):.2f}]")
        print(f"  Mean: {np.mean(values):.2f}, Median: {np.median(values):.2f}")
        print(f"  Std Dev: {np.std(values):.2f}")
        
        # Show what happens if we normalize these values
        normalized = normalize_0_100(values)
        print(f"  After normalize_0_100: [{min(normalized):.2f}, {max(normalized):.2f}]")
        print(f"  (This would map minâ†’0, maxâ†’100)")


def simulate_batch_scoring(df, titles_to_test):
    """
    Simulate what title_scoring_helper.py would produce for a specific batch.
    
    This demonstrates the key issue: batch normalization vs. global scores.
    """
    print("\n" + "="*80)
    print("BATCH SCORING SIMULATION")
    print("="*80)
    
    # Filter to just the titles being scored
    batch = df[df['title'].isin(titles_to_test)].copy()
    
    print(f"\nScoring batch of {len(batch)} titles:")
    for title in titles_to_test:
        print(f"  - {title}")
    
    # Simulate fetching "raw" values (in reality these come from APIs)
    # For simulation, we'll use the baseline values as proxies
    batch['wiki_raw'] = batch['wiki']
    batch['trends_raw'] = batch['trends']
    batch['youtube_raw'] = batch['youtube']
    batch['chartmetric_raw'] = batch['chartmetric']
    
    # Apply normalization (this is what title_scoring_helper.py does)
    batch['wiki_normalized'] = normalize_0_100(batch['wiki_raw'].tolist())
    batch['trends_normalized'] = normalize_0_100(batch['trends_raw'].tolist())
    batch['youtube_normalized'] = normalize_0_100(batch['youtube_raw'].tolist())
    batch['chartmetric_normalized'] = normalize_0_100(batch['chartmetric_raw'].tolist())
    
    print("\n" + "-"*80)
    print("COMPARISON: Baseline vs. Batch-Normalized")
    print("-"*80)
    
    for _, row in batch.iterrows():
        print(f"\n{row['title']}:")
        for signal in ['wiki', 'trends', 'youtube', 'chartmetric']:
            baseline = row[signal]
            normalized = row[f'{signal}_normalized']
            diff = normalized - baseline
            print(f"  {signal:8s}: baseline={baseline:6.2f}, normalized={normalized:6.2f}, diff={diff:+7.2f}")


def check_for_precision_issues(df):
    """Check if there are any precision/rounding differences."""
    print("\n" + "="*80)
    print("PRECISION ANALYSIS")
    print("="*80)
    
    for signal in ['wiki', 'trends', 'youtube', 'chartmetric']:
        # Check decimal precision
        decimals = df[signal].dropna().apply(lambda x: len(str(x).split('.')[-1]) if '.' in str(x) else 0)
        
        print(f"\n{signal.upper()}:")
        print(f"  Max decimal places: {decimals.max()}")
        print(f"  Values with decimals: {(decimals > 0).sum()} / {len(decimals)}")
        print(f"  Integer values: {(decimals == 0).sum()} / {len(decimals)}")


def check_data_sources(df):
    """Analyze differences between historical and reference titles."""
    print("\n" + "="*80)
    print("DATA SOURCE ANALYSIS")
    print("="*80)
    
    historical = df[df['source'] == 'historical']
    reference = df[df['source'] == 'external_reference']
    
    print(f"\nHistorical titles: {len(historical)}")
    print(f"Reference titles: {len(reference)}")
    
    for signal in ['wiki', 'trends', 'youtube', 'chartmetric']:
        print(f"\n{signal.upper()}:")
        print(f"  Historical - Mean: {historical[signal].mean():.2f}, Range: [{historical[signal].min():.2f}, {historical[signal].max():.2f}]")
        print(f"  Reference  - Mean: {reference[signal].mean():.2f}, Range: [{reference[signal].min():.2f}, {reference[signal].max():.2f}]")


def identify_root_causes():
    """Summarize the root causes of discrepancies."""
    print("\n" + "="*80)
    print("ROOT CAUSE SUMMARY")
    print("="*80)
    
    print("""
The primary issue causing score mismatches is:

ðŸ”´ BATCH-RELATIVE NORMALIZATION vs. GLOBAL SCORES

title_scoring_helper.py normalizes scores RELATIVE to the current batch:
  1. User enters N titles
  2. Fetch raw API values for those N titles
  3. Apply normalize_0_100() to just those N values
  4. Min value in batch â†’ 0, Max value in batch â†’ 100

baselines.csv contains GLOBAL scores:
  - Scores are normalized across ALL 114+ titles
  - A title's score depends on where it falls in the full distribution
  - Min across all titles â†’ 0, Max across all titles â†’ 100

EXAMPLE:
  If you score ["Cinderella", "Swan Lake"] together:
    - One will get 0, the other will get 100 (assuming different raw values)
  
  But in baselines.csv:
    - Cinderella: wiki=80, Swan Lake: wiki=80 (both high but not extremes)
    - Because they're compared against ALL titles including low-scoring ones

OTHER POTENTIAL ISSUES:
  âœ“ Precision: baselines.csv has mix of integers and decimals (not an issue)
  âœ“ Rounding: Some variation but not systematic (minor)
  âœ“ Data sources: Both historical and reference use same methodology (OK)
  âœ— Batch normalization: THIS IS THE MAIN PROBLEM

RECOMMENDATION:
  The title_scoring_helper.py should normalize against a reference distribution
  (e.g., all historical titles) rather than just the current batch.
""")


def main():
    """Run all diagnostic checks."""
    print("="*80)
    print("TITLE SCORING MISMATCH DIAGNOSTIC")
    print("="*80)
    
    # Load data
    df = load_baselines()
    print(f"\nLoaded {len(df)} titles from baselines.csv")
    
    # Run analyses
    analyze_normalization_impact(df)
    
    # Simulate scoring a batch
    test_titles = ["Cinderella", "Swan Lake", "The Nutcracker", "Giselle"]
    available_titles = [t for t in test_titles if t in df['title'].values]
    if available_titles:
        simulate_batch_scoring(df, available_titles)
    
    check_for_precision_issues(df)
    check_data_sources(df)
    identify_root_causes()
    
    print("\n" + "="*80)
    print("DIAGNOSTIC COMPLETE")
    print("="*80)
    print("\nNext steps:")
    print("  1. Review the ROOT CAUSE SUMMARY above")
    print("  2. Consider using a reference distribution for normalization")
    print("  3. See suggested fixes in the output")
    print("="*80)


if __name__ == "__main__":
    main()
