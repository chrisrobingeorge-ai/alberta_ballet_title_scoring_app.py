ALBERTA BALLET TITLE SCORING APP
Technical Overview for Executive Leadership

The Alberta Ballet Title Scoring App is a machine learning decision support system that predicts ticket demand for proposed ballet titles. The fundamental challenge: programming seasons 12 to 18 months in advance with limited data on how audiences will respond.

The system generates a Ticket Index score on a 0 to 100-plus scale, translating raw predictions into six defined demand tiers from 'emerging' below 60 to 'exceptional' above 120. These are demand priors, not guarantees—quantitative forecasts that inform artistic decisions without replacing judgment.

The prediction engine synthesizes four digital signals, manually curated and stored in the baselines database covering 288 reference titles.

First: Wikipedia pageview index. Average daily pageviews from January 2020 to present, log-scaled using the formula: 40 plus the minimum of 110 and natural log of 1 plus views times 20. This produces a range of 40 to 150.

Second: Google Trends search volume from January 2022 forward, normalized using Giselle as a bridge calibration control with a known score of 14.17 to align disparate batch collections.

Third: YouTube engagement index from January 2023 onward. View counts are indexed against Cinderella as benchmark, then log-scaled: 50 plus the minimum of 90 and natural log of 1 plus indexed views times 9. Category-specific winsorization at the 3rd and 97th percentiles prevents viral videos from distorting forecasts.

Fourth: Chartmetric streaming index covering the last two years. Weighted artist rank scores from Spotify, Apple Music, Amazon, Deezer, YouTube Music, Shazam, TikTok, Instagram, Twitter, and Facebook, inverted and normalized to a 0 to 100 scale.

These four signals aggregate into two composite indices.

Familiarity measures public recognition: Wikipedia times 0.55 plus Trends times 0.30 plus Chartmetric times 0.15. Wikipedia receives heavy weighting as it represents static knowledge—'I've heard of this ballet.'

Motivation measures active engagement intent: YouTube times 0.45 plus Trends times 0.25 plus Chartmetric times 0.15 plus Wikipedia times 0.15. YouTube dominates because viewing behavior indicates 'I want to see this.'

The SignalOnly composite combines these equally: 0.50 times Familiarity plus 0.50 times Motivation. Typical range: 20 to 180. This becomes the primary predictor variable feeding the machine learning models.

The system implements a four-tier fallback strategy to ensure predictions are always available.

Tier One: Historical data. For remounts, the system looks up median ticket sales from prior runs stored in the baselines dictionary covering 282 reference productions.

Tier Two: Dynamically-trained Ridge Regression models. When users supply historical production data, the system trains category-specific models at runtime if five or more samples exist per category. Overall models train with three or more total samples. Training uses constrained Ridge regression with synthetic anchor points—SignalOnly equals zero anchored to Ticket Index 25, and SignalOnly equals 100 anchored to Ticket Index 100—weighted by the maximum of 3 or half the real sample count. Alpha parameter of 5.0 provides regularization. This pulls predictions toward realistic endpoints while fitting actual historical patterns.

If insufficient data exists for Ridge, the system falls back to LinearRegression with the same anchor constraints.

Tier Three: k-Nearest Neighbors cold-start fallback. Cosine similarity matching against baseline signals with k equals 5 neighbors. Distance-weighted voting applies recency decay at 0.1 per year. The formula: combined weight equals similarity times the quantity recency weight times e to the negative lambda times years ago, plus 1 minus recency weight. This ensures even completely new titles receive predictions anchored to similar historical performances.

Tier Four: Signal-Only estimate. If no historical or model data exists, the system defaults to the raw SignalOnly composite.

The model outputs a deseasonalized Ticket Index. Seasonality factors are learned from historical data: for each category-month combination with at least 3 samples, the system calculates median sales divided by overall category median.

Shrinkage applies: 1 plus 3.0 times the quantity raw factor minus 1. This dampens extreme adjustments from limited data. Factors are clipped to 0.90 floor and 1.15 ceiling—negative 10 percent penalty to plus 15 percent boost.

The effective Ticket Index equals the deseasonalized prediction times the future seasonality factor. A show scoring 100 in a neutral month becomes 120 if scheduled in December for the family category.

City decomposition: The system learns Calgary-Edmonton splits from historical data, title-specific when available, category-level otherwise, defaulting to 60-40. Splits are clipped between 40 and 75 percent to prevent unrealistic allocations. Geographic multipliers apply: Calgary 1.05, Edmonton 0.95.

Segment propensity: Four audience segments—General Population, Core Classical females 35 to 64, Family parents with children, and Emerging Adults 18 to 34. Allocation combines prior weights from historical attendance with signal-based affinity using softmax-like normalization. Each production generates 8 ticket estimates: 4 segments times 2 cities.

Performance evaluation uses five-fold time-aware cross-validation. This strictly chronological approach ensures models train only on past data and validate on future data, mirroring real-world deployment conditions.

Legacy XGBoost metrics from 25 post-COVID training samples: Mean Absolute Error 696 tickets with standard deviation 366. This means predictions typically differ from actuals by about 696 tickets. For a 2,000-ticket production, that's 35 percent error; for a 10,000-ticket production, 7 percent error.

Root Mean Squared Error: 822 tickets, standard deviation 376. RMSE penalizes large errors more heavily.

R-squared: 0.800, standard deviation 0.134. The model explains approximately 80 percent of variance in ticket sales—strong performance for cultural demand forecasting. The remaining 20 percent reflects both irreducible audience behavior stochasticity and patterns not yet captured.

Individual fold results show R-squared ranging from 0.65 to 0.91. The high variance in Mean Absolute Error—plus or minus 50 percent—indicates sensitivity to show type. Best performance appears in recent folds, suggesting the model benefits from recency.

Current production models use dynamically-trained Ridge Regression, which adapts to the specific characteristics of user-supplied historical data. Performance varies based on data quantity and quality.

Every prediction includes a SHAP explanation—Shapley Additive exPlanations from game theory. SHAP values decompose each forecast, showing which factors influenced the prediction and by how much.

The implementation at lines 2830 through 4140 of the main application file generates these explanations automatically. For a proposed title, SHAP quantifies the contribution of each signal: 'Wikipedia pageviews increased the prediction by 8 index points. YouTube engagement decreased it by 3 points. Historical median performance from prior runs contributed 15 points.'

This provides transparency into the model's reasoning. Data scientists can validate the logic. Executives can understand the drivers. Artistic leadership can assess whether the factors align with their strategic vision.

Feature importance metrics complement SHAP values. The economics system—consumer confidence, energy index, inflation adjustment—contributes approximately 1.2 index points but provides the largest correlation impact: negative 0.104 when removed. Live analytics engagement contributes 3.2 points with negative 0.008 correlation gain. The Stone Olafson hard-coded segment multipliers contribute 2.6 points but show zero correlation improvement, suggesting they may not reflect actual behavior.

Critical boundaries: The system predicts ticket volume, not revenue. It does not account for pricing strategies, discount structures, or complimentary policies. It does not learn from marketing campaign outcomes or optimize scheduling. Retraining requires manual invocation—the model does not update in real time.

Known data quality issues: Consumer confidence has only 1 to 2 unique values across the dataset. Live analytics engagement has only 4 unique values, limiting discriminative power.

The Title Scoring App provides evidence-based demand forecasts. These are informed starting points for decision-making. The system combines historical patterns, digital signals, economic indicators, and learned seasonality to translate uncertainty into quantified priors. The methodology is transparent, the math is auditable, and the predictions are explainable. This is decision support, not decision replacement—analytical rigor serving artistic vision.
