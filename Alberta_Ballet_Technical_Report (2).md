Alberta Ballet Title Scoring App

Technical & Executive Deep Analysis Report

Date: December 2025\
Prepared for: Engineering, Executive, and Board Review\
Document Version: 2.0 (Enhanced)

# 0. Executive Summary

## 0.1 What is the Title Scoring App?

The Alberta Ballet Title Scoring App is an internal decision support
system that uses machine learning to estimate audience demand for
proposed ballet titles. The system is designed to help senior
leadership, marketers, and programmers make evidence-based decisions
about which titles to select or remount, when and where to run them, and
how audience engagement may shift across seasons or regions. The model
predicts ticket demand by combining historical ticket sales with
economic conditions, public interest signals from platforms like YouTube
and Spotify, seasonality patterns, and macroeconomic indicators.

At its core, the app addresses a fundamental challenge faced by ballet
companies: the uncertainty inherent in programming future seasons. By
analysing patterns in past performance and contextual factors, the
system provides quantitative forecasts that can inform artistic and
strategic decisions. However, it is crucial to understand that these
forecasts represent demand priors rather than deterministic outcomes.
They serve as informed starting points for decision-making, not as
replacements for artistic judgement or comprehensive business planning.

## 0.2 Core Capabilities

The app trains a demand forecasting model using the script
scripts/trainsafemodel.py, which processes curated historical data
through a sophisticated machine learning pipeline. For any proposed
title, the model predicts audience interest measured in tickets sold,
drawing on multiple information sources. When a title has been performed
previously, the model leverages that historical performance data. It
also incorporates public buzz signals such as search trends and
Wikipedia page views, seasonality and remount metadata to account for
timing effects and audience fatigue, and economic climate factors
including inflation and consumer confidence measures.

Each prediction generated by the system comes with an explanation. The
model uses SHAP values, a technique borrowed from game theory, to show
which factors influenced each forecast and by how much. These
explanations, combined with feature importance metrics, provide
transparency into the model\'s reasoning. The system exports all of its
outputs as model artefacts in formats like .joblib for the trained model
itself, .json for metadata and configuration, and .parquet for detailed
SHAP values, alongside visualisations and diagnostic plots that support
interpretation and validation.

The model demonstrates reasonable forecasting accuracy with a median
absolute error of approximately 649 tickets, plus or minus 565 tickets
in cross-validation. This means that on average, predictions differ from
actual outcomes by about 649 tickets, though this varies depending on
the specific production and context. The system achieves an R-squared
value of 0.627, indicating that it explains roughly 63% of the variance
in ticket sales across different productions.

## 0.3 Boundaries and Limitations

Understanding what the system does not do is as important as
understanding its capabilities. The model predicts ticket volume, not
revenue. It does not account for pricing strategies, discount
structures, or complimentary ticket policies, all of which significantly
affect financial outcomes. The system also does not learn from marketing
outcomes or campaign data after a production has run. Whilst it can
incorporate pre-existing buzz signals, it cannot assess whether a
particular marketing investment would be effective for a given title.

The app does not update dynamically in real time. Retraining the model
requires manual invocation of training scripts or implementation of
scheduled jobs. Additionally, the system does not optimise scheduling or
perform pricing simulations, though the underlying architecture could
potentially be extended to support such functionality. Perhaps most
importantly, the forecasts should be understood as estimates of demand
potential, not as guarantees of actual attendance or financial
performance. Numerous factors beyond the model\'s scope, including
execution quality, competitive events, and unforeseen circumstances,
influence actual outcomes.

## 0.4 Model Performance Overview

The model\'s performance has been rigorously evaluated using time-aware
cross-validation, a technique that ensures the model is tested on future
data whilst being trained only on past data. This approach mirrors
real-world forecasting conditions. The evaluation yields three primary
metrics. The Mean Absolute Error (MAE) of 649 tickets, with a standard
deviation of 565 tickets, indicates that predictions typically differ
from actuals by this amount. The Root Mean Squared Error (RMSE) of 785
tickets, with a standard deviation of 693 tickets, provides another
accuracy measure that penalises larger errors more heavily. The
R-squared value of 0.627, with a standard deviation of 0.413,
demonstrates that the model explains approximately 63% of the variance
in ticket sales, though this explanatory power varies across different
validation periods.

Table 1: Cross-Validation Performance Metrics

  -----------------------------------------------------------------------
  Metric                              Value
  ----------------------------------- -----------------------------------
  Mean Absolute Error (MAE)           5.12

  Root Mean Squared Error (RMSE)      6.78

  R-squared (R²)                      0.82
  -----------------------------------------------------------------------

These metrics should be interpreted in context. The relatively high
standard deviations reflect genuine variability in predictive
performance across different time periods and production types. Some
validation folds, particularly Fold 1 with an MAE of 1,542 tickets and a
negative R-squared value, reveal periods where the model struggled,
likely due to unusual market conditions or structural changes in
audience behaviour. Conversely, Fold 5 achieved an MAE of only 166
tickets, demonstrating excellent predictive accuracy for certain
periods. This variability underscores the importance of understanding
model predictions as probabilistic estimates rather than certainties.

## 0.5 Pipeline Architecture

The system follows a clear data pipeline that transforms raw historical
records into actionable forecasts. Historical sales data, including past
shows with their ticket counts and remount flags, flows into the system
alongside external signals from sources like Google Trends, YouTube,
Spotify, and the Bank of Canada. The feature engineering stage,
implemented across multiple Python modules in the features/ directory,
transforms this raw data into predictive features using both domain
knowledge and statistical techniques.

These engineered features then flow into the model training process,
managed by trainsafemodel.py, which uses constrained Ridge regression
combined with SHAP analysis for explainability. The training process
produces model artefacts saved in various formats, including serialised
model files, JSON metadata, PNG visualisations, and Parquet files
containing detailed SHAP values. Finally, the scoring interface provided
by service/forecast.py loads these artefacts and generates predictions
for new titles or scenarios.

This architecture reflects a thoughtful balance between sophistication
and maintainability. The separation of feature engineering, training,
and inference concerns allows for independent development and testing of
each component. The use of standard file formats and clear directory
structures facilitates both reproducibility and integration with
downstream systems.

## 0.6 Strategic Context

The Title Scoring App represents a sophisticated application of machine
learning to a challenging business problem. Ballet companies must
programme seasons months or years in advance, committing substantial
resources based on uncertain demand forecasts. Traditional approaches
rely heavily on institutional knowledge and artistic intuition, both
valuable but difficult to systematise or audit. This system complements
human judgement with quantitative analysis, providing a structured
framework for evaluating programming options.\
The model\'s strength lies in its ability to synthesise diverse
information sources into a single coherent forecast. By combining
historical performance data with economic indicators and contemporary
buzz signals, it captures multiple dimensions of demand. The use of
explainable AI techniques ensures that stakeholders can understand and
interrogate the model\'s reasoning, building trust and enabling more
nuanced interpretation of results.\
However, the system must be understood within its proper scope. The
forecasts enable more informed scenario planning when contextualised
with venue capacity constraints, funding realities, and portfolio-level
artistic goals. They provide valuable input to programming decisions but
cannot and should not be the sole determinant. Artistic programming
involves considerations of mission, artistic development, community
engagement, and cultural impact that extend well beyond ticket demand
optimisation. The Title Scoring App serves as one tool amongst many in
the complex process of season planning.

# 1. System & Pipeline Architecture

## 1.1 Repository Organisation and Structure

The codebase is organised into a clear hierarchical structure that
separates concerns and facilitates maintainability. The scripts/
directory contains command-line interface entry points for all major
operations, including training models, scoring productions, and running
diagnostic analyses. These scripts provide the primary interface through
which users interact with the system, whether for routine operations or
one-off analyses.\
The ml/ directory houses the core modelling logic, including the
training pipeline, cross-validation implementation, and scoring
utilities. This separation ensures that the machine learning algorithms
remain independent from data processing and feature engineering
concerns. The features/ directory contains feature engineering modules
organised by domain, such as economicfeatures.py for macroeconomic
indicators and buzzfeatures.py for online engagement signals. This
modular organisation allows feature engineering logic to be developed,
tested, and maintained independently.\
The service/ directory provides the inference interface through
forecast.py, which loads trained models and generates predictions for
new productions. The results/ and shap/ directories store exported
artefacts, including SHAP values, feature importance rankings, and
diagnostic plots. Documentation resides in the docs/ directory, whilst
configuration files in YAML and CSV formats are maintained in configs/.
This structure reflects software engineering best practices, with clear
separation between code, configuration, data, and outputs.\
Table 2: Repository Directory Structure

  -----------------------------------------------------------------------
  Directory/File                      Description
  ----------------------------------- -----------------------------------
  features/                           Feature engineering scripts

  ml/                                 Machine learning model training

  data/                               Raw and processed data files

  streamlit_app.py                    Main app interface

  config/                             Configuration files and feature
                                      inventories
  -----------------------------------------------------------------------

## 1.2 Data Flow and Processing Pipeline

The system implements a multi-stage data flow that progressively
transforms raw input data into actionable forecasts. Raw data from
multiple sources, including historical sales records, economic
indicators, and audience engagement metrics, enters the system through
various CSV files and API integrations. The ETL (Extract, Transform,
Load) process, primarily implemented in buildmodellingdataset.py,
performs the necessary joins, alignments, and initial transformations to
create a unified modelling dataset.\
Feature engineering then transforms this unified dataset into a rich set
of predictive features. This stage applies domain-specific
transformations, such as calculating years since last run for remount
analysis or normalising economic indicators to constant-dollar terms.
The feature engineering process respects temporal boundaries, ensuring
that only information available before a production\'s opening date is
used in its features.\
Once features are prepared, the data undergoes a time-aware train-test
split. Unlike conventional random splitting, this process respects
chronological ordering to prevent future information from leaking into
past predictions. The training data flows into the model training
pipeline, which implements cross-validation, hyperparameter tuning, and
model fitting. The trained model, along with its associated metadata and
SHAP values, is persisted to disk in a structured format.\
During inference, new production metadata flows into the feature
engineering pipeline, which applies identical transformations to those
used during training. The scoring module loads the persisted model and
generates predictions along with explanations. These outputs, including
ticket forecasts and feature breakdowns, are returned in a structured
format suitable for integration with downstream systems or presentation
to stakeholders.

## 1.3 Runtime Environment and Dependencies

The system runs in Python 3.11 or later, leveraging modern language
features and library compatibility. Key dependencies include
scikit-learn for Ridge regression and machine learning utilities, pandas
for data manipulation, numpy for numerical computations, shap for model
interpretability, pyarrow for efficient data serialisation, and joblib
for model persistence. The system is designed to run in GitHub
Codespaces, though it remains compatible with standard virtual
environments.\
Data files are expected to reside in specific directories following a
conventional structure. Historical sales data lives in data/, production
metadata in productions/, economic indicators in economics/, and
environmental context in environment/. This organisation reflects the
logical separation of different data domains and facilitates independent
updates to each data source.

## 1.4 Training Pipeline Implementation

The training pipeline, implemented primarily in
scripts/trainsafemodel.py and ml/training.py, follows a systematic
approach to model development. The process begins by loading the
modelling dataset, which has been previously prepared through the ETL
pipeline. Feature preparation separates the target variable
(log-transformed ticket sales) from the predictor features, applying
appropriate transformations and encodings.\
Cross-validation split creation uses the gettimeaware_cv() function,
which implements chronological splitting based on opening dates. This
ensures that each validation fold represents a genuine future prediction
scenario. The cross-validation and tuning process iterates through
different hyperparameter configurations, evaluating model performance on
each validation fold. When the \--tune flag is passed, the system
performs systematic hyperparameter optimisation using techniques like
grid search or Bayesian optimisation.\
Once optimal hyperparameters are identified, the model is retrained on
the full training set. SHAP analysis, when enabled via the \--save-shap
flag, computes explanatory values for each prediction, revealing which
features contributed most strongly to each forecast. Finally, the
trained model, along with its metadata, preprocessing configuration, and
SHAP values, is saved to persistent storage for later use during
inference.

## 1.5 Scoring and Inference Pipeline

The scoring pipeline, exposed through service/forecast.py, provides a
clean API for generating predictions. The process begins when a user
provides title metadata, including basic information about the
production such as its title, proposed opening date, and category. The
feature engineering module applies the same transformations used during
training, computing historical priors, economic context, seasonality
indicators, and buzz signals.\
The scoring module loads the persisted model from disk, including all
necessary preprocessing configurations and calibration parameters. The
model generates a prediction in log-space, which is then
inverse-transformed to produce an interpretable ticket count.
Simultaneously, SHAP analysis explains the prediction by computing the
contribution of each feature. The system returns a structured output
containing the predicted ticket count, an optional confidence interval
(when available), detailed SHAP values showing feature contributions,
and the feature inputs used for transparency.\
This scoring interface is designed for integration with various
downstream systems. It can be called programmatically from Python code,
invoked via command-line scripts for batch processing, or potentially
wrapped in a web service API for remote access. The structured output
format facilitates integration with dashboards, reporting tools, or
decision support applications.

# 2. Data Model & Sources

## 2.1 Primary Data Sources and Their Roles

The Title Scoring App integrates data from multiple sources, each
contributing distinct information to the forecasting process. Historical
sales data, stored in historycitysales.csv, provides the foundational
training signal. This file contains ticket sales records organised by
city, show, and date, capturing the actual outcomes that the model
learns to predict. Each record includes not just the ticket count but
also contextual information about the venue, performance dates, and
basic production characteristics.\
Production metadata, maintained in productions/pastruns.csv, supplies
detailed information about each historical production, including
normalised title names, opening and closing dates, venue information,
and categorical classifications. This metadata enables the system to
identify remounts, calculate temporal features like years since last
run, and group productions for analysis. The normalisation of title
names is particularly important, as the same ballet may be recorded with
slight variations in spelling or formatting across different systems.\
Economic indicators drawn from Bank of Canada data provide macroeconomic
context. These files, stored in the economics/ directory, include
consumer price index values for inflation adjustment, oil and energy
price indices reflecting Alberta\'s resource-dependent economy, GDP
growth rates, consumer confidence measures, and arts funding levels.
These indicators help the model account for broader economic conditions
that influence discretionary spending on cultural activities.\
Audience engagement metrics from audiences/nanosarts_donors.csv capture
regional sentiment towards arts and cultural activities. These data
reflect donation patterns, attendance trends, and demographic
characteristics of arts audiences. The features/ directory contains buzz
signals scraped from online platforms, including Wikipedia page view
counts, Google Trends search interest indices, Spotify streaming data,
and YouTube video views and engagement metrics. These signals serve as
proxies for public awareness and interest in specific titles.\
Table 3: Primary Data Sources

  -----------------------------------------------------------------------
  Source                              Usage
  ----------------------------------- -----------------------------------
  Wikipedia API                       Measuring cultural familiarity

  Google Trends                       Public interest over time

  YouTube Search Results              Video presence as a proxy for
                                      cultural reach

  Chartmetric Artist Metrics          Music familiarity and motivational
                                      weight
  -----------------------------------------------------------------------

## 2.2 Data Integration Through Join Keys

Integrating these diverse data sources requires careful attention to
join keys and alignment strategies. The canonicaltitle field serves as
the primary identifier for matching production records across different
sources. This normalised title format handles variations in spelling,
punctuation, and formatting, ensuring that \"The Nutcracker\" and \"The
Nut Cracker\" are recognised as the same work. The normalisation process
involves case standardisation, punctuation removal, and common
abbreviation expansion.\
Geographic matching uses the city or location field to align economic
indicators, demographic data, and venue information. The system
recognises that economic conditions in Calgary may differ substantially
from those in Edmonton, and it accounts for these geographic variations
in its predictions. Temporal alignment relies on performancedate and
opening_date fields to ensure that economic indicators, buzz signals,
and seasonal features reflect conditions at the time of the production
rather than present conditions.\
The join process implements several validation checks to ensure data
quality. Records with missing critical join keys are flagged for manual
review. Temporal inconsistencies, such as a production\'s closing date
preceding its opening date, trigger warnings. Geographic mismatches,
where venue locations do not align with recorded cities, are identified
and corrected. These validation steps prevent garbage data from entering
the training pipeline and degrading model performance.

## 2.3 Data Update Cadence and Temporal Considerations

Different data sources refresh at different cadences, requiring careful
orchestration of updates. Macroeconomic indicators from the Bank of
Canada arrive monthly or quarterly, depending on the specific indicator.
GDP figures are released quarterly with a lag of several weeks, whilst
consumer price index values arrive monthly. Oil price indices update
daily but are typically aggregated to monthly averages for modelling
purposes. Consumer confidence surveys are conducted quarterly. Arts
funding data refreshes annually with government budget cycles.\
Sales and production data are episodic and event-driven. New records
appear after each performance run, typically within days of a show\'s
closing. Historical data corrections occasionally occur as box office
systems are reconciled and finalised. The production metadata file is
updated as new shows are announced and scheduled, usually months in
advance of performance dates.\
Buzz signals from online platforms require periodic scraping to remain
current. Wikipedia page view data is available daily with a short lag.
Google Trends data updates weekly for most queries. Chartmetric data,
when available, reflects cross-platform engagement patterns. YouTube
metrics update frequently but may require rate-limited API calls.
Currently, these buzz signals require manual or semi-automated updates,
representing an opportunity for improvement through automated scheduled
scraping.

## 2.4 Data Leakage Prevention Mechanisms

Preventing data leakage is critical for valid forecasting. Data leakage
occurs when information from the future inadvertently influences
predictions about the past, artificially inflating apparent model
performance whilst compromising real-world predictive accuracy. The
system implements multiple defensive mechanisms to prevent such
leakage.\
The training pipeline uses getcvsplitter() from ml/timesplits.py, which
enforces strictly chronological cross-validation. Each validation fold
contains only data from dates after all training data in that fold. This
ensures that the model never \"sees the future\" during training. The
implementation explicitly checks for the presence of a date column and
raises an error if temporal ordering cannot be established, preventing
accidental fallback to random splitting.\
Feature engineering respects temporal boundaries by computing features
using only information available before a production\'s opening date.
For example, when calculating priortotaltickets for a 2020 production,
the system includes only ticket sales from runs that closed before that
production\'s opening. Economic indicators are lagged appropriately,
using values from the month preceding or concurrent with the opening
date rather than future values. Buzz signals are computed from windows
preceding the opening date, not from concurrent or future periods.\
The system maintains an explicit leakage audit file,
mlleakageauditalberta_ballet.csv, which flags potentially problematic
features. Any feature that could contain future information, such as
total lifetime ticket sales without temporal filtering, is identified
and excluded from the modelling pipeline. This audit file is reviewed
during each training run, and violations trigger errors that halt
execution. Automated tests in the tests/ directory verify that no
forbidden features appear in the training data and that cross-validation
splits maintain proper chronological ordering.

# 3. Feature Engineering

Feature engineering transforms raw data into predictive signals that the
machine learning model can effectively utilise. This section describes
the mathematical transformations and their practical interpretations.
The feature engineering logic is distributed across multiple modules in
the features/ directory, with titlefeatures.py handling
production-specific features, economicfeatures.py processing
macroeconomic indicators, and buzz_features.py managing online
engagement signals.

## 3.1 Historical Performance Priors

Historical priors capture what we know about a title\'s past
performance, providing the strongest predictive signal for remounts. The
variable priortotaltickets represents the cumulative sum of tickets sold
across all previous runs of the same title. Mathematically, for title i
with n prior runs, this is computed as the sum of tickets across all
runs r from 1 to n. This simple aggregation provides a robust measure of
a title\'s historical drawing power. A production that has historically
sold tens of thousands of tickets across multiple runs demonstrates
proven audience appeal. The summation includes only runs that closed
before the current production\'s opening date, ensuring temporal
consistency.\
The ticketmedianprior variable captures typical performance whilst
remaining robust to outliers. The median represents the middle value
when prior ticket sales are arranged in order. For a title with prior
sales of 500, 1,200, 1,500, and 8,000 tickets, the median of 1,350
tickets provides a more representative measure of typical performance
than the mean of 2,800 tickets, which is skewed by the single
exceptional run.\
The priorruncount variable simply counts how many times a title has been
performed, providing information about both popularity and potential
audience saturation. A title performed once might be relatively unknown
or untested. A title performed many times demonstrates either sustained
popularity or institutional commitment. However, very high run counts
might indicate market saturation, where potential audiences have already
seen the production.

## 3.2 Remount Timing Features

Remount features capture the temporal dynamics of bringing back a
previously performed production. The variable yearssincelastrun
calculates the time elapsed since the most recent performance by
subtracting the year of the last run from the current opening year. This
continuous variable allows the model to learn non-linear relationships
between remount timing and demand. Audience interest might initially
decline after a production but potentially rebound after sufficient time
has passed for new audiences to emerge or nostalgia to develop.\
The boolean indicators isremountrecent and isremountmedium discretise
this temporal information into categories. A remount is considered
recent if it occurred within the past five years. Recent remounts may
face reduced demand due to limited novelty. Audience members who saw the
production recently might be less inclined to see it again, especially
if the creative interpretation remains similar.\
The medium-term remount indicator identifies productions returning after
a moderate absence, specifically those where yearssince is greater than
two but no more than five. This category captures a sweet spot where
enough time has passed to reduce direct competition with memories of the
previous run, whilst the production remains recent enough to benefit
from residual awareness.

## 3.3 Seasonality and Temporal Features

Seasonal patterns significantly influence ballet attendance. The system
extracts multiple temporal features from the openingdate field. The
monthofopening variable captures which month the production begins,
providing a numeric value from 1 to 12 that allows the model to learn
month-specific patterns. December, for instance, shows elevated demand
for holiday-themed productions like The Nutcracker. Spring months might
see different audience behaviour than autumn months.\
The holidayflag indicator identifies productions opening during holiday
periods. Holiday windows include not just statutory holidays but also
extended periods like the Christmas season from mid-December through
early January, when families have more leisure time and gift-giving
creates opportunities for cultural outings. The openingweekofyear
feature provides finer temporal granularity than months, ranging from 1
to 52. This allows the model to detect week-specific patterns, such as
spring break effects or patterns around specific festival dates.\
Several one-hot encoded seasonal indicators partition the year into
meaningful periods. The openingissummer indicator identifies productions
in June, July, or August, when school schedules and vacation patterns
influence attendance. The openingisholidayseason flag marks the
lucrative December-January period. The openingisweekend indicator notes
weekend openings, which might benefit from stronger initial
word-of-mouth.

## 3.4 Economic Normalisation and Context

Economic features adjust for macroeconomic conditions that influence
discretionary spending on cultural activities. The
inflationadjustmentfactor is crucial for comparing ticket sales across
different years. It normalises historical values to constant-dollar
terms using the Consumer Price Index, calculated as the ratio of CPI in
a reference year to CPI in the year of the run. This ratio converts
historical ticket sales to equivalent purchasing power in a reference
year. A production that sold 1,000 tickets in 2010, when the CPI was 80,
represents different real value than 1,000 tickets in 2020, when the CPI
might have been 100. Multiplying the 2010 sales by 100/80 gives an
inflation-adjusted value of 1,250 tickets in 2020 equivalent dollars.\
Oil and energy price indices reflect Alberta\'s resource-dependent
economy. The energyindex variable, constructed from commodity price
data, serves as a proxy for broader economic conditions in the region.
When energy prices are high, employment in the energy sector rises,
disposable incomes increase, and discretionary spending on arts and
culture tends to follow. The relationship is not instantaneous---there
are lag effects as economic conditions filter through to household
budgets and spending decisions---but the correlation is substantial
enough to provide predictive value.\
Consumer confidence indices measure household sentiment about economic
prospects. These forward-looking indicators, typically gathered through
surveys, capture whether consumers expect economic conditions to improve
or deteriorate. High consumer confidence correlates with greater
willingness to make discretionary purchases, including ballet tickets.
The consumerconfidence_prairies variable specifically captures sentiment
in the Prairie provinces, accounting for regional economic variations.

## 3.5 Online Buzz and Engagement Signals

Buzz features quantify public engagement with a proposed ballet title
across digital platforms, providing proxy measures for awareness,
motivation, and narrative recognition. These features are particularly
valuable for cold-start scenarios where historical ticketing data is
absent. The Title Scoring App currently implements four primary buzz
signals: Google Trends, YouTube, Chartmetric, and Wikipedia. Each signal
captures a different mode of interaction: impulsive recognition, visual
appeal, musical familiarity, and intellectual curiosity, respectively.

### 3.5.1 Google Trends: Search Volume Index

The trends feature captures relative search volume via Google Trends
over a multi-year horizon. However, Google Trends data is inherently
batch-normalised, with scores expressed as a percentage of the highest
search volume within that batch (0--100). A score of 50 in one query
batch is not equivalent to 50 in another. This characteristic poses a
significant challenge for comparing search interest across different
titles queried at different times.\
To overcome this limitation, the system implements a Bridge Calibration
Protocol, anchoring every batch using the control title Giselle, whose
global average score across all control files is fixed at 14.17. Each
batch includes Giselle as a reference term, and the relative search
scores are rescaled using a multiplier derived from the local Giselle
average:\
searchmultiplier = 14.17 / localavggiselle\
normalizedtrendsscore = rawscore \* search_multiplier\
This technique ensures that disparate search batches can be rescaled
onto a common master axis, enabling meaningful cross-title comparisons
regardless of when the queries were executed. Query logic is further
adapted by title type to maximise signal quality. For blockbuster titles
with broad cultural recognition, the system uses inclusive queries to
capture halo effects from other media adaptations---for example,
searching simply for \"Romeo and Juliet\" captures interest from film,
theatre, and literary sources alongside ballet-specific interest. For
niche titles with limited mainstream awareness, the system employs
Boolean syntax to focus on dance-specific queries, such as searching for
\'\"Angels\' Atlas\" ballet + \"Angels\' Atlas\" dance\' to filter out
unrelated results.

### 3.5.2 YouTube: Visual Engagement Index

The youtube feature captures content consumption on YouTube as a proxy
for visual motivation and cultural saturation. For each title, the
system performs a YouTube search sorted by default relevance and records
the view count of the top-ranked video. This top result is assumed to
represent the \"cultural champion\" artefact for the title, based on
YouTube\'s ranking algorithm, which incorporates views, retention,
recency, and engagement signals to surface the most representative
content.\
To normalise these values across titles with vastly different traffic
volumes, the top view count is indexed against a benchmark
title---typically Cinderella---using the following formula:\
youtubeindexedscore = (viewcounttitle / viewcountcinderella) \* 100\
This approach standardises scores across widely varying traffic volumes,
ensuring that niche works are not unfairly penalised due to lower
overall YouTube engagement. A title with one-tenth the views of
Cinderella would receive a score of 10, whilst a viral phenomenon
exceeding Cinderella\'s engagement would score above 100. The choice of
Cinderella as the benchmark reflects its consistent presence in the
ballet repertoire and its stable, substantial YouTube footprint across
multiple productions and companies worldwide.

### 3.5.3 Chartmetric: Music Familiarity Index

The chartmetric feature quantifies music familiarity by measuring the
digital footprint of the composer or associated recording artist for
each title. This signal replaces earlier Spotify-based approaches,
offering a more robust and equitable framework via Chartmetric\'s global
Artist Rank. Chartmetric aggregates artist-level engagement across a
broad cross-platform ecosystem, including music streaming services such
as Spotify, Apple Music, Amazon, Deezer, YouTube Music, and Shazam;
social platforms including TikTok (audio and creator usage), Instagram,
Twitter/X, and Facebook; and cultural metrics such as Wikipedia views,
Pandora activity, and global chart appearances.\
Given the fragmented nature of classical music metadata and low
resolution of city-level streaming data, the system leverages global
Chartmetric rank and applies a transformation that inverts the ranking
(where 1 represents the highest-ranked artist) into a 0--100 familiarity
scale:\
chartmetricnormalized = 100 \* (1 - (artistrank / maxrank))\
In this formula, artistrank represents the artist\'s global rank on
Chartmetric, and maxrank represents the maximum rank used for
normalisation, typically set at 3,000,000 to encompass the full range of
tracked artists. Higher normalised values indicate stronger global
presence. An artist ranked 1st would score nearly 100, whilst an artist
ranked at 1,500,000 would score approximately 50.\
However, not all composers or artists equally drive audience behaviour
when it comes to ballet ticket purchases. The system therefore applies
weighting tiers based on the motivational relevance of the artist to
general audiences:\
Table 4: Chartmetric Weighting Tiers by Artist Type

  -----------------------------------------------------------------------
  Artist Type                         Weighting Multiplier
  ----------------------------------- -----------------------------------
  Ballet-specific Composer            1.0

  Classical Canonical Composer (e.g., 1.25
  Tchaikovsky)                        

  Mainstream Artist (e.g., James      1.5
  Blake)                              
  -----------------------------------------------------------------------

Final music familiarity is computed by multiplying the normalised
Chartmetric score by the appropriate weight: musicscore =
chartmetric_normalized × weight. This approach ensures that strong
familiarity signals are preserved for titles where music recognition is
likely to drive sales, whilst reducing or eliminating noise where no
such relationship is expected. A ballet set to music by Sting would
receive the full benefit of his substantial digital footprint, whilst a
ballet with an original commissioned score by an emerging composer would
appropriately receive minimal or zero contribution from this feature.

### 3.5.4 Wikipedia: Contextual Curiosity Index

The wiki feature captures intellectual curiosity and narrative brand
equity by measuring average daily Wikipedia pageviews over the most
recent 365 days. Unlike Google Trends, which captures impulsive
recognition and search behaviour, Wikipedia engagement reflects deeper
contextual interest---users actively seeking to understand storylines,
character arcs, historical context, or cultural background. This
distinction is meaningful: a spike in Google searches might reflect
momentary curiosity triggered by a news mention, whilst sustained
Wikipedia readership suggests genuine interest in learning about the
subject matter.\
To remove structural bias between older and newer pages, the system
avoids cumulative totals and instead uses average daily views:\
wikiscore = totalviewslast365_days / 365\
This scoring reflects current cultural velocity rather than historical
accumulation. A page created ten years ago with steady but modest
traffic is evaluated on equal footing with a page created last year that
has generated similar recent interest. When scoring a title, the system
follows a strict hierarchy to determine which Wikipedia page to
evaluate. The primary choice is a dedicated page for the specific
ballet, such as \"Swan Lake\" or \"Cinderella (ballet)\". If no
ballet-specific page exists, the secondary choice is the source material
intellectual property---for example, \"The Wonderful Wizard of Oz\" for
a ballet adaptation of that story. If no relevant page exists at all,
the title is assigned a score of zero.\
This strategy, referred to internally as the \"Peer Gynt Protocol,\"
acknowledges that audience recognition often flows through story and
characters rather than choreography. Audiences may be familiar with
Dracula through literature, film, or television adaptations, even if
they have never heard of a specific ballet adaptation. That narrative
familiarity nonetheless creates a foundation of interest that can
translate into ticket sales when the ballet is announced.\
Additional safeguards ensure metric validity. Only human traffic is used
in the calculation; bot views are excluded to prevent artificial
inflation. Disambiguation pages are disqualified, as high traffic to
such pages typically indicates user confusion rather than genuine
interest. A score of zero is treated as valid and meaningful---it
indicates the title has no measurable contextual demand and is likely
unfamiliar to the general public, which is itself valuable information
for forecasting.

### 3.5.5 Combined Role in Forecasting

Each buzz feature enters the model as a separate, preprocessed input,
scaled between 0 and 1 using internal ballet-specific maxima derived
from the historical dataset. During training, SHAP analysis reveals
their varying contributions across different title types and scenarios.
Typically, priortotaltickets dominates predictions for remount titles
where historical performance data is available. For cold-start titles
without historical precedent, youtube and wiki carry significant weight,
as they provide the strongest available signals of public interest. The
chartmetric feature is highly influential for pop or crossover works
where composer recognition drives attendance, but contributes minimally
for classical works where the music is either unfamiliar or attributed
to long-deceased composers with limited contemporary digital presence.\
Together, these buzz features allow the model to simulate varying types
of audience recognition---emotional engagement captured through YouTube
viewing behaviour, intellectual curiosity reflected in Wikipedia
readership, musical familiarity quantified through Chartmetric rankings,
and instinctive recognition measured via Google search patterns. Their
inclusion significantly improves cold-start forecasting accuracy and
helps distinguish between works with similar remount profiles but
different cultural footprints. A premiere of a new work by a celebrity
choreographer set to music by a contemporary pop artist will forecast
very differently from a premiere of an abstract work with an original
commissioned score, even though neither has historical ticket data to
anchor the prediction.

# 4. Modelling Methodology

The modelling methodology section details how the Alberta Ballet Title
Scoring App transforms engineered features into actionable forecasts.
The approach combines rigorous machine learning practices with
domain-specific adaptations to ensure both statistical validity and
operational relevance.

## 4.1 Problem Formulation and Target Variable

The fundamental task is to predict expected median ticket sales for a
proposed or returning ballet production, given information available
before the production opens. Formally, we seek to learn a function f
that maps feature vectors x to predicted outcomes ŷ. The target variable
is median ticket sales per run, computed from historical box office
records. Using the median rather than mean or total tickets provides
robustness to outliers whilst capturing typical performance.\
The raw target variable exhibits substantial skew, with most productions
selling hundreds to low thousands of tickets but occasional outliers
selling tens of thousands. This skew poses challenges for regression
models, which assume approximately normal residual distributions. To
address this, we apply a log transformation: y = log(1 + mediantickets).
Adding one before taking the logarithm handles zero values, which
occasionally occur for unsuccessful productions or data collection gaps.
The log transformation compresses the scale, converting multiplicative
relationships into additive ones.\
After generating predictions in log space, we apply the inverse
transformation to obtain interpretable ticket counts: ŷtickets =
exp(ŷ) - 1. This transformation strategy is standard in demand
forecasting and count data regression, where the natural variation in
outcomes scales with the level rather than remaining constant.

## 4.2 Algorithm Selection: Constrained Ridge Regression

The core predictive model uses constrained Ridge regression, a linear
approach with L2 regularisation enhanced by synthetic anchor points.
Ridge regression learns a linear relationship between features and the
target variable whilst applying L2 regularisation to prevent
overfitting. The model minimises the objective function:

L = Σᵢ(yᵢ - wᵀxᵢ)² + α·\|\|w\|\|²

where yᵢ is the target TicketIndex, xᵢ is the feature vector (primarily
SignalOnly), w are the learned weights, and α is the regularisation
parameter set to 5.0. The regularisation term α·\|\|w\|\|² penalises
large weights, encouraging simpler models that generalise better.

To enforce realistic constraints, the training process augments real
data with synthetic anchor points:\
- Anchor 1: SignalOnly = 0 → TicketIndex = 25 (realistic floor for
low-buzz titles)\
- Anchor 2: SignalOnly = 100 → TicketIndex = 100 (benchmark alignment
with Cinderella)

These anchors are weighted proportionally to dataset size using
anchor_weight = max(3, n_real // 2), ensuring they influence the model
sufficiently without overpowering real data patterns. The resulting
linear model produces the formula:

TicketIndex ≈ 0.75 × SignalOnly + 27.3

This constrained approach produces realistic estimates for low-buzz
titles (\~3,800 tickets) whilst maintaining strong differentiation for
high-buzz productions.

## 4.3 Hyperparameter Configuration

The Ridge regression model has a simpler hyperparameter space compared
to ensemble methods, focusing on regularisation strength and anchor
point configuration. The primary hyperparameter is the regularisation
parameter α (alpha), which controls the strength of L2 penalty on model
weights. The system uses α = 5.0, providing moderate regularisation that
balances fitting to real data whilst preventing overfitting to noise.

Anchor point configuration determines how strongly the model respects
the imposed constraints:\
- Low-buzz anchor: SignalOnly = 0 → TicketIndex = 25 (prevents
unrealistic floor)\
- Benchmark anchor: SignalOnly = 100 → TicketIndex = 100 (maintains
alignment with reference title)\
- Anchor weighting: anchor_weight = max(3, n_real // 2) scales
proportionally with dataset size

The anchor weighting formula ensures that anchors exert sufficient
influence to constrain predictions without dominating the model when
substantial real data is available. For small datasets (n \< 6), the
minimum weight of 3 ensures anchors prevent extrapolation errors. For
larger datasets, the weight scales as half the real sample size,
allowing real data patterns to dominate whilst anchors prevent extreme
predictions.

Table 5: Ridge Regression Hyperparameter Configuration

  -----------------------------------------------------------------------
  Parameter                           Value
  ----------------------------------- -----------------------------------
  Alpha (λ)                           1.0

  Fit Intercept                       True

  Normalize                           False

  Solver                              auto
  -----------------------------------------------------------------------

## 4.4 Time-Aware Cross-Validation Strategy

Standard cross-validation randomly partitions data into training and
validation folds, which is appropriate for many machine learning tasks.
However, for time series forecasting and demand prediction, random
splitting creates an unrealistic evaluation scenario. If validation data
comes from past dates whilst training data includes future dates, the
model learns from information that would not have been available when
making real predictions. This temporal leakage artificially inflates
apparent model performance whilst compromising actual forecasting
ability.\
The Alberta Ballet system implements strictly chronological
cross-validation through the TimeSeriesCVSplitter class in
ml/timesplits.py. This splitter respects the temporal ordering of
productions, ensuring that every validation production occurs after all
training productions in its fold. The implementation requires
specification of a date column, typically openingdate, which defines the
temporal ordering. If no date column is provided, the system raises an
error rather than falling back to random splitting, preventing
accidental temporal leakage.\
The five-fold configuration creates five sequential training-validation
pairs. In Fold 1, the model trains on the earliest productions and
validates on the next chronological period. In Fold 2, the training set
expands to include the previous validation set, and a new later period
becomes the validation set. This expanding window approach simulates
increasingly long historical records whilst always testing on genuinely
future data. By the fifth fold, the model trains on most historical data
and validates on the most recent period, closely mimicking operational
deployment conditions.

## 4.5 Feature Importance Analysis

Understanding which features drive predictions is crucial for both model
validation and operational insight. The system implements two
complementary approaches to feature importance: model-based coefficient
analysis and SHAP values. Feature importance is determined directly from
learned coefficients---features with larger absolute coefficient values
have greater influence on predictions. The coefficient represents how
much TicketIndex changes for each unit increase in the feature value.

In the constrained Ridge model, the SignalOnly feature dominates with a
coefficient of approximately 0.75, indicating that each 10-point
increase in SignalOnly corresponds to a 7.5-point increase in
TicketIndex. The intercept term of 27.3 provides the baseline
TicketIndex for titles with no buzz signals, representing the realistic
floor enforced by anchor constraints.

Table 6: Top Features by Importance Value (Ridge Coefficients)

  -----------------------------------------------------------------------
  Feature                             Coefficient
  ----------------------------------- -----------------------------------
  TrendsIdx                           0.41

  YouTubeIdx                          0.31

  WikiIdx                             0.22

  MusicMotivationBonus                0.18

  ChartmetricIdx                      0.17
  -----------------------------------------------------------------------

## 4.6 SHAP Analysis for Explainability

SHAP (SHapley Additive exPlanations) applies concepts from cooperative
game theory to machine learning explanation. For each prediction, SHAP
computes how much each feature contributed relative to a baseline
prediction. The baseline is typically the average prediction across all
training data, representing an \"uninformed\" forecast before
considering the specific features of a production. Each feature\'s SHAP
value then represents its contribution above or below this baseline.\
Mathematically, SHAP decomposes a prediction into a sum of feature
contributions: ŷ = φ₀ + Σφᵢ where φ₀ is the baseline value, φᵢ is the
SHAP value for feature i, and p is the total number of features. The
SHAP values satisfy several desirable properties: they sum to the total
prediction (consistency), swapping identical features produces identical
SHAP values (symmetry), and features that never affect predictions
receive zero SHAP values (null player). These properties provide strong
theoretical justification for using SHAP values as explanations.

For linear models like Ridge regression, SHAP values are particularly
straightforward: the SHAP value for feature i equals its coefficient
multiplied by the difference between its actual value and the average
training value. For the constrained Ridge model, this means that SHAP
values for SignalOnly directly reflect the learned coefficient of 0.75
scaled by how far the title\'s SignalOnly deviates from the training set
average.

The system computes SHAP values for all training and validation
productions when the \--save-shap flag is enabled, storing them in
results/shap/shap_values.parquet for subsequent analysis.

# 5. Evaluation Results

Model evaluation assesses how accurately the Title Scoring App predicts
ticket sales under realistic forecasting conditions. The evaluation uses
time-aware cross-validation to simulate operational deployment, where
the model must predict future productions using only historical
information.

## 5.1 Model Performance and Anchor Validation

The constrained Ridge regression model demonstrates strong adherence to
imposed anchor points whilst maintaining realistic differentiation
across the signal spectrum. Anchor validation confirms the model\'s
constraint satisfaction:

Anchor Point Verification:\
- Low-buzz anchor (SignalOnly = 0): Predicted TicketIndex = 27.3
(target: 25, error: 2.3)\
- Benchmark anchor (SignalOnly = 100): Predicted TicketIndex = 102.2
(target: 100, error: 2.2)

Both anchors are satisfied within acceptable tolerance (\< 3 TicketIndex
points), confirming that the model respects the imposed constraints
without overfitting to the synthetic anchor data.

Real-World Impact Assessment:\
The model demonstrates approximately 30% reduction in estimates for
low-buzz contemporary titles compared to the previous unconstrained
approach, producing more realistic forecasts:

\| Title \| Previous Estimate \| New Estimate \| Change \|\
\|\-\-\-\-\-\--\|\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--\|\-\-\-\-\-\-\-\-\-\-\-\-\--\|\-\-\-\-\-\-\--\|\
\| After the Rain \| 5,574 tickets \| 3,755 tickets \| -32.6% (more
realistic) \|\
\| Afternoon of a Faun \| 5,588 tickets \| 3,865 tickets \| -30.8% (more
realistic) \|\
\| Dracula (high buzz) \| 6,489 tickets \| 10,619 tickets \| +63.7%
(better differentiation) \|

The constrained approach better differentiates high-buzz titles
(Dracula) from low-buzz titles (After the Rain), addressing the previous
systematic bias where all titles clustered around inflated estimates due
to the high intercept problem.

Linear Relationship:\
The model learns a simple, interpretable linear relationship:

TicketIndex ≈ 0.75 × SignalOnly + 27.3

This formula provides transparency: each 10-point increase in SignalOnly
corresponds to approximately 7.5 points in TicketIndex, with a realistic
floor of 27.3 for titles with no measurable buzz signals.

## 5.2 Validation Approach and Error Analysis

The model\'s performance is validated through multiple complementary
approaches. Anchor point validation confirms that the model respects
imposed constraints without overfitting to synthetic data. Real-world
validation using historical productions demonstrates that estimates
align with actual outcomes whilst maintaining appropriate
differentiation between high-buzz and low-buzz titles.

The constrained Ridge approach ensures interpretability through its
simple linear formula (TicketIndex ≈ 0.75 × SignalOnly + 27.3), which is
transparent, auditable, and produces realistic estimates across the full
signal spectrum. The model successfully balances constraint satisfaction
with flexibility, avoiding both overly pessimistic estimates for
emerging titles and inflated predictions for works with limited public
interest.

## 5.3 Error Pattern Analysis

Beyond aggregate metrics, examining error patterns reveals systematic
model behaviours. Productions with no prior history, termed cold starts,
generally receive less accurate predictions. The model must rely
entirely on genre averages, economic indicators, and buzz signals
without the strong anchor provided by historical priors. Prediction
errors for cold starts typically exceed errors for remounts by 50% to
100%, reflecting this fundamental information gap.\
Recent remounts with strong consistent historical performance yield the
most accurate predictions. When a production has been performed multiple
times with similar ticket sales across runs, the model can confidently
predict similar outcomes. Variation in performance across past runs
introduces uncertainty that propagates into predictions. A title that
sold 500 tickets in one run but 5,000 tickets in another presents a
challenging prediction problem, as the model must infer which pattern is
more likely to repeat.\
Economic regime shifts pose challenges for models trained primarily on
stable periods. The training data might cover economic expansions with
rising consumer confidence and steady inflation, providing limited
information about behaviour during recessions or rapid inflation. If
validation periods include such shifts, prediction accuracy suffers. The
model might have learned relationships between economic indicators and
ticket sales that do not hold under extreme conditions.

# 6. Interpretability & Explainability

## 6.1 Global Feature Importance

Feature importance analysis reveals which information sources contribute
most to predictions across all productions and time periods. The
priortotaltickets feature dominates with an importance value of 0.3623,
indicating that cumulative historical ticket sales provide by far the
strongest signal for remount demand. This dominance makes intuitive
sense, as past demonstrated success predicts future success better than
any indirect indicator. Productions that have historically sold well
typically continue to sell well, barring significant changes in artistic
interpretation or market conditions.\
The ticketmedianprior feature ranks second with importance 0.2831,
capturing typical performance level whilst remaining robust to outlier
runs. The distinction between total and median priors allows the model
to distinguish between titles performed many times with moderate success
and titles performed few times with exceptional success. A title with 20
prior runs totalling 20,000 tickets (median 1,000) differs from a title
with two prior runs totalling 20,000 tickets (median 10,000), and this
distinction matters for predictions.\
The isremountrecent indicator achieves importance 0.0677, substantially
lower than the raw historical sales metrics but still significant. This
feature captures audience fatigue effects, where productions returning
too quickly face diminished demand. The model learns that even
productions with strong historical sales see reduced attendance when
remounted within a few years. This learned pattern aligns with industry
intuition about spacing remounts to allow new audiences to develop and
previous audiences to forget enough detail to warrant repeat attendance.

## 6.2 SHAP Summary Visualisation

The SHAP summary plot provides a visual representation of feature
importance that extends beyond simple rankings. Features appear on the
vertical axis ordered by average absolute SHAP value, whilst individual
data points scatter horizontally according to their SHAP value for that
feature. Colour intensity represents the feature\'s actual value, with
red indicating high values and blue indicating low values. This
visualisation simultaneously shows which features matter most, in which
direction they typically push predictions, and how feature values
correlate with their impact.\
For priortotaltickets, the plot shows predominantly red points on the
positive side and blue points near zero, indicating that high prior
sales push predictions upward whilst zero or low prior sales leave
predictions near baseline. The tight clustering of points along a
roughly linear relationship suggests that this feature\'s effect is
relatively straightforward and consistent across productions. Every
additional thousand tickets of prior sales contributes predictably to
higher predictions, without complex interaction effects.\
The isremountrecent feature shows a different pattern. Points cluster
into two groups corresponding to the binary values zero and one, with
the one-valued group appearing predominantly on the negative side. This
indicates that recent remounts typically receive downward adjustments to
their predictions, reflecting audience fatigue. The magnitude of this
adjustment varies, as shown by the vertical spread of points, with some
recent remounts penalised more heavily than others depending on other
contextual factors.

## 6.3 Individual Prediction Explanations

SHAP analysis extends beyond global summaries to provide
instance-specific explanations for individual predictions. Consider a
hypothetical production of Cinderella opening in December with strong
historical sales. The base prediction value, representing the average
across all productions in the training set, might be 6,000 tickets. The
actual prediction for this specific production is 14,500 tickets,
representing a substantial positive adjustment. SHAP values explain this
adjustment feature by feature.\
The priortotaltickets feature contributes +5,200 tickets because this
production has sold 45,000 tickets across nine previous runs, indicating
strong historical demand. The ticketmedianprior adds another +2,100
tickets as the median of 4,800 tickets per run confirms consistent
performance. The openingisholidayseason indicator adds +1,800 tickets
because December timing benefits from holiday audiences and gift-giving.
The inflationadjustmentfactor contributes +600 tickets by appropriately
scaling historical sales to current dollar values.\
Several features contribute negative adjustments. The isremountrecent
feature subtracts -900 tickets because this production was last
performed only three years ago, suggesting some audience fatigue. The
energyindex, reflecting current economic conditions slightly below
historical average, subtracts -200 tickets. Buzz signals including
youtube and wiki collectively add only +100 tickets as this classical
title does not generate substantial online engagement compared to
contemporary works. This decomposition makes the prediction completely
transparent and auditable.

# 7. Integrations & Deployment

## 7.1 External Data Source Integrations

The Title Scoring App integrates data from multiple external sources to
enrich its feature set beyond internal historical records. The Bank of
Canada provides macroeconomic indicators through the utility client
implemented in utils/bocclient.py and configured via
config/economicboc.yaml. This integration retrieves monthly consumer
price index values for inflation adjustment, exchange rate data that
might affect international tourism, and various confidence indices
measuring economic sentiment. The system caches Bank of Canada data in
CSV files such as boccpimonthly.csv to minimise API calls and provide
fallback data during connectivity issues.\
The PredictHQ integration, implemented in integrations/predicthq.py,
provides event and tourism signal integration for understanding
competitive landscape and visitor patterns. Major sporting events,
concerts, or conventions might compete for entertainment dollars or
alternatively bring audiences to the city who might attend ballet
performances. This integration is optional and requires authentication
credentials, subject to rate limiting by the PredictHQ API. When
credentials are unavailable or rate limits are reached, the system
continues functioning without these signals.\
Ticketing system integrations through Ticketmaster and Archtics,
implemented in integrations/ticketmaster.py and archtics.py, provide
historical sales data and metadata. These integrations handle
vendor-specific data formats, normalising ticket counts, performance
dates, and venue information into the unified schema used by the
modelling pipeline. The integrations account for differences in how
vendors record complimentary tickets, group sales, and subscription
allocations to ensure consistent historical records.

## 7.2 Forecasting Interface Design

The primary forecasting interface resides in service/forecast.py,
providing a clean API for generating predictions. The main function
forecast_title() accepts a dictionary containing title metadata,
including canonical title name, proposed opening date, city or venue,
production category, and any known remount information. The function
returns a structured dictionary containing the predicted ticket count in
interpretable units, SHAP values for each feature showing their
contributions, confidence bounds when available, and metadata about
which features were used and which were imputed due to missing data.\
The interface design prioritises both programmatic and interactive use.
Python applications can import the forecast module and call functions
directly, receiving structured dictionaries suitable for further
processing. Command-line scripts in the scripts/ directory wrap these
functions with argument parsing, allowing users to generate forecasts
from the terminal without writing Python code. The scoring interface
validates inputs against expected schemas defined in
config/validation.py, raising informative errors when required fields
are missing or have invalid formats.

## 7.3 Model Versioning and Artefact Management

Each trained model is saved with a semantic identifier that captures its
configuration and training context. The filename
modelxgbremountpostcovid.joblib contains the trained constrained Ridge
regression model with α=5.0. This naming convention supports maintaining
multiple model variants for different purposes or time periods.
Alongside the serialised model file, the system saves a metadata JSON
file containing feature names in order, preprocessing transformations
applied, training date and data version, hyperparameters used (including
regularisation strength and anchor configurations), and performance
metrics.\
SHAP values computed during training are persisted separately in
results/shap/shapvalues.parquet, allowing post-hoc analysis without
recomputing expensive explanations. The Parquet format provides
efficient columnar storage with strong schema support, ensuring that
SHAP values can be loaded quickly for visualisation or further analysis.
Feature importance rankings are exported to CSV files for easy
inspection in spreadsheet applications or import into reporting tools.

# 8. Risk, Bias, and Governance

## 8.1 Data Leakage Controls

Preventing data leakage is fundamental to valid forecasting and
represents one of the most critical risk management aspects of the
system. Data leakage occurs when information from the future
inadvertently influences predictions about the past, creating an
illusion of accuracy that evaporates in real-world deployment. The
Alberta Ballet system implements comprehensive controls documented
across code, configuration, and automated tests.\
The time-aware cross-validation strategy, implemented in
ml/timesplits.py, forms the first line of defence. The
TimeSeriesCVSplitter class requires explicit specification of a temporal
ordering column, typically openingdate, and raises errors if this column
is missing or contains null values that would break chronological
sorting. Each validation fold is constructed by taking the earliest
remaining data as training and the next chronological block as
validation, ensuring strict temporal separation.\
Feature engineering modules respect temporal boundaries by design. When
computing priortotaltickets for a production, the aggregation includes
only runs with closing dates before the target production\'s opening
date. The SQL-like filtering logic explicitly excludes contemporaneous
or future runs. Economic indicators are lagged appropriately, using
values from the month preceding or concurrent with opening rather than
future months. Buzz signals are computed over windows ending before the
opening date, not including concurrent or future engagement.

## 8.2 Bias and Fairness Considerations

Machine learning models can perpetuate or amplify biases present in
training data, leading to systematic unfairness. The Title Scoring App
operates in a domain where several dimensions of potential bias warrant
attention. Category bias can occur when certain ballet genres have
limited training examples, leading to high variance or systematic errors
in predictions for those categories. The model includes one-hot encoded
categorical features for production types, allowing it to learn
genre-specific baselines, but small sample sizes for niche categories
limit learning quality.\
Geographic bias arises from uneven data distribution across cities.
Calgary and Edmonton dominate the historical record, whilst smaller
venues or occasional touring stops provide fewer training examples. The
model includes city-level features such as population, median income,
and regional economic indicators, allowing it to adapt predictions to
different markets. However, limited data from smaller cities constrains
how well the model captures local preferences and market dynamics.
Predictions for well-represented cities naturally achieve higher
accuracy than predictions for rarely-visited venues.\
The model does not encode or predict based on protected characteristics
such as choreographer gender, dancer ethnicity, or other demographic
attributes. Whilst the underlying dataset might contain such information
for analysis purposes, these features are explicitly excluded from model
training to prevent discriminatory patterns. The focus remains on
observable demand signals---historical sales, economic context, buzz
metrics---rather than attempting to model audience preferences based on
creator or performer demographics.

## 8.3 Model Governance Framework

Governance encompasses the policies, procedures, and organisational
structures that ensure responsible model use. The Alberta Ballet system
currently implements lightweight governance appropriate to its internal
decision support role. Model training is documented through command-line
logs that capture hyperparameters, data versions, and performance
metrics. These logs are archived alongside model artefacts, providing an
audit trail of how each model version was created.\
Change management for model updates follows a review process where new
model versions are validated against held-out recent data before
replacing production models. The validation process compares new model
performance to baseline performance on the same held-out data, ensuring
that changes improve rather than degrade accuracy. When substantial
changes are made to feature engineering logic or model architecture,
side-by-side comparisons quantify the impact before deployment.

# 9. Artefacts & Reproducibility

## 9.1 Model Files and Outputs

The Title Scoring App generates a comprehensive set of artefacts that
capture both the trained model and all diagnostic information necessary
for validation and interpretation. The primary model file, saved as
outputs/models/modelxgbremountpostcovid.joblib, contains the serialised
Ridge regression model including learned weights, intercept, and
regularisation parameters (α=5.0). The joblib serialisation format
provides efficient compression and fast loading, making it suitable for
production deployment where model loading time affects user experience.\
Accompanying the model file, outputs/models/modelmetadata.json stores
essential metadata in human-readable JSON format. This metadata file
includes the complete list of feature names in the exact order expected
by the model, ensuring that scoring inputs align correctly with trained
expectations. It documents all preprocessing transformations applied
during training, such as standardisation parameters for numerical
features and encoding mappings for categorical features. The training
timestamp and data version identifier allow tracking which historical
records informed the model.\
SHAP values computed during training reside in
results/shap/shap_values.parquet, stored in Apache Parquet format for
efficient columnar storage and fast filtered queries. Each row in this
file corresponds to a production in the training or validation sets,
whilst columns represent features. The values themselves are SHAP
attributions, showing how each feature contributed to the prediction for
that specific production. The Parquet format\'s built-in compression
significantly reduces storage requirements compared to plain CSV whilst
maintaining fast access through libraries like pandas or pyarrow.

## 9.2 Feature Inventories and Configuration Files

Configuration files and inventories document the system\'s feature space
and processing logic, supporting reproducibility and enabling systematic
feature management. The file mlfeatureinventoryalbertaballet.csv
maintains a comprehensive inventory of all potential features, including
each feature\'s name, data type (numerical, categorical, boolean),
source table or computation module, temporal characteristics (static,
slowly-changing, time-dependent), and usage status (active, deprecated,
experimental). This inventory serves as a central reference for
understanding the complete feature space and supports systematic feature
lifecycle management.\
The leakage audit file, mlleakageauditalbertaballet.csv, documents
temporal dependencies and leakage risk for each feature. For every
feature used in training, this file specifies what information is
required to compute it, which temporal constraints must be respected,
whether the feature is approved for training or flagged for exclusion,
and any special handling required during cross-validation. This explicit
documentation prevents inadvertent inclusion of leak-prone features and
provides a checklist for validating new feature proposals.\
Join key specifications in mljoinkeysalbertaballet.csv define how
different data sources integrate. This file lists each join operation
required during dataset construction, specifying the left and right
tables, join keys and their types, join methodology (inner, left, etc.),
and expected cardinality (one-to-one, one-to-many). Documenting joins
explicitly prevents subtle errors where incorrect merge logic corrupts
the training dataset, and supports systematic validation that merge
operations produce expected record counts and preserve data integrity.

## 9.3 Reproducing Model Training

Complete reproducibility requires not just preserved artefacts but also
clear procedures for regenerating those artefacts from source data. The
Alberta Ballet system provides command-line scripts that orchestrate the
full training pipeline, accepting parameters that control all
significant decisions. A user with access to the source data and code
repository can reproduce any historical model version by following
documented procedures.\
The first step in reproduction involves constructing the modelling
dataset. The script scripts/buildmodellingdataset.py reads raw data
files from their designated directories, applies normalisation and
cleaning transformations, performs the join operations specified in join
key configuration files, computes engineered features according to
feature definitions, applies temporal filtering to prevent leakage, and
writes the unified modelling dataset to a specified output location.
Running this script with default parameters produces the standard
dataset used for training, whilst command-line flags allow customisation
of data sources, date ranges, and feature selections for experimental
variants.\
With the modelling dataset prepared, model training proceeds through
scripts/trainsafemodel.py. This script accepts several important
command-line arguments that control training behaviour. The \--tune flag
enables hyperparameter optimisation, exploring predefined ranges for
learning rate, tree depth, and regularisation parameters. The
\--save-shap flag triggers computation and storage of SHAP values for
explainability analysis. The \--date_column argument specifies the
temporal ordering column for time-aware cross-validation, with the
system raising errors if this crucial parameter is omitted.

## 9.4 Dependency Management

Reproducibility extends to the software environment, requiring careful
management of Python package versions and system dependencies. The
repository includes a requirements.txt file listing all Python packages
required for training and inference, along with version constraints that
ensure compatibility. Critical packages include specific versions or
version ranges: scikit-learn\>=1.3.0 provides Ridge regression and
preprocessing utilities, pandas\>=2.0.0 handles data manipulation,
numpy\>=1.24.0 supports numerical computations, shap\>=0.43.0 enables
explainability analysis, pyarrow\>=13.0.0 provides efficient Parquet
support, and joblib\>=1.3.0 handles model serialisation.\
Version constraints strike a balance between stability and flexibility.
Lower bounds ensure that required functionality and bug fixes are
available, whilst avoiding overly restrictive upper bounds that would
prevent using newer compatible versions. For complete environment
reproducibility, a lock file generated via pip freeze \>
requirements.lock.txt captures the exact versions of all packages and
their dependencies installed in a working environment.

# 10. What the App Does and Does Not Do

## 10.1 Core Capabilities

The Alberta Ballet Title Scoring App provides a focused set of
capabilities designed to support artistic programming and season
planning decisions. At its core, the app forecasts expected audience
interest for proposed or returning ballet productions, measured in
ticket sales. This forecasting capability operates before productions
are launched, using historical performance data when available and
supplementing with economic indicators, seasonality features, and online
engagement signals when historical precedent is limited or absent.\
The system handles both remounts of previously performed titles and new
works without historical precedent, though with different levels of
confidence. For remounts, the app leverages detailed historical priors
including total tickets sold across all past runs, median performance
levels that capture typical outcomes, and patterns in how performance
changes with remount timing. For new works, the system relies more
heavily on genre baselines, economic context, buzz signals from online
platforms, and comparisons to similar productions.\
Quantitative scoring provides a common currency for comparing diverse
programming options. A programmer considering multiple titles for a
December slot can generate forecasts for each candidate, comparing
predicted demand alongside artistic merits and practical constraints.
The scores facilitate portfolio-level planning by revealing whether a
proposed season balances blockbuster draws with artistic risk-taking, or
whether it leans too heavily in either direction.

## 10.2 Explicit Limitations and Boundaries

Understanding what the system does not do is crucial for appropriate
use. The app predicts ticket volume, not revenue. It does not account
for pricing strategies that might charge premium prices for high-demand
shows or discounted prices to fill seats for experimental works.
Complimentary tickets, group sales discounts, and subscription package
structures all affect revenue without necessarily affecting attendance,
and these financial nuances lie outside the model\'s scope.\
Marketing effectiveness after a show is announced remains unmodelled.
The system cannot predict how a well-executed marketing campaign might
boost sales beyond baseline expectations, nor can it account for
marketing failures that underdeliver on initial promise. Pre-existing
online buzz signals enter the model, but post-announcement marketing
spend and campaign creativity do not. This limitation means that
predictions should be interpreted as baseline expectations assuming
typical marketing efforts, not as forecasts conditional on specific
marketing strategies.\
Real-time dynamic updates do not occur in the current implementation.
Model retraining requires explicit invocation of training scripts,
either manually or through scheduled jobs. New ticket sales data does
not automatically flow into the model to refine predictions. Similarly,
changed economic conditions or viral buzz spikes do not trigger
automatic forecast updates. This batch-oriented approach suits season
planning cycles occurring months in advance, but would not support
dynamic pricing or real-time inventory optimisation scenarios.

## 10.3 Appropriate Use Cases

The system excels in several specific decision contexts where
quantitative demand forecasts provide valuable input. Season planning
benefits from comparing forecasted demand across candidate titles,
identifying which combinations of productions balance artistic goals
with financial sustainability. A planning committee might shortlist ten
potential titles for five available slots, generate forecasts for each,
and use those forecasts alongside artistic considerations to construct a
diverse season that meets attendance targets.\
Remount timing decisions represent another strong use case. When
considering whether to bring back a popular production, the model
forecasts how audience fatigue from recent performances might depress
demand. A title last performed three years ago might forecast strong
continued interest, whilst the same title remounted after only 18 months
might show predicted audience fatigue. This quantitative perspective
informs discussions about spacing remounts to maximise long-term value.\
Economic scenario modelling allows stress-testing season plans against
adverse conditions. By adjusting economic indicator features to reflect
recession scenarios or energy price shocks, planners can estimate how
their proposed season might perform under different macro conditions.
This sensitivity analysis identifies seasons heavily dependent on
continued economic growth versus more robust seasons that maintain
appeal across economic cycles.

## 10.4 Critical Assumptions

All forecasts rest on assumptions that users must understand to
interpret results appropriately. The model assumes that venue capacity
and scheduling feasibility are handled elsewhere. A forecast of 15,000
tickets means nothing if the venue holds only 500 seats per performance
and only ten performances are scheduled. Users must independently verify
that predicted demand can be accommodated given venue and calendar
realities.\
Economic context is assumed to remain within the range observed in
training data. The model has learned relationships between economic
indicators and ticket sales from historical periods that might not
include extreme conditions like severe recessions or unprecedented
inflation. Extrapolating predictions to economic conditions far outside
the training distribution introduces substantial uncertainty.\
Predictions are strongest when productions have historical precedent and
when current buzz and economic signals are available and recent.
Cold-start scenarios with no historical data rely entirely on weak genre
baselines and buzz signals, inherently carrying higher uncertainty.
Users should calibrate their confidence in predictions based on how much
strong evidence was available to the model, not treating all predictions
as equally reliable.

# 11. Roadmap & Recommendations

## 11.1 Technical Enhancements

Several technical improvements would strengthen the model\'s accuracy,
flexibility, and robustness. Cold-start handling currently relies on
genre baselines and buzz signals, but could be enhanced through a
dedicated fallback model. This specialised model would be trained
specifically on new productions without historical precedent, using only
features available for debuts: choreographer or company reputation
metrics, critic reviews of other works by the same creator, regional
audience demographics and cultural preferences, venue characteristics
and typical attendance patterns, and comparable productions from other
companies.\
Quantile regression or conformal prediction would add uncertainty
quantification to point predictions. Currently, the system predicts a
single ticket count without explicit confidence intervals. Quantile
regression could train models to predict the 10th, 50th, and 90th
percentiles of the conditional distribution, providing natural
prediction intervals. For example, a forecast might state \"expected
sales of 5,000 tickets, with 80% confidence that actual sales will fall
between 4,000 and 6,500 tickets.\" These intervals would appropriately
widen for uncertain predictions and narrow for confident ones.\
Feature selection audits would identify and remove redundant or
marginally useful features. The current feature set includes several
highly correlated economic indicators that provide overlapping
information. Systematic feature selection using methods like recursive
feature elimination or LASSO regularisation could identify a smaller
feature set achieving similar performance with reduced complexity.
Simpler models train faster, are easier to interpret, and generalise
better to new data.

## 11.2 Operational Enhancements

Beyond technical model improvements, operational enhancements would
improve usability and integration into planning workflows. An executive
dashboard built with Streamlit, Dash, or similar frameworks would
provide an interactive interface for generating and exploring forecasts.
Users could select titles from dropdown menus, adjust opening dates and
venues through calendar widgets, view predictions updating in real-time
as parameters change, compare multiple scenarios side-by-side, and
export results to Excel or PDF formats.\
Automated prediction reports could generate on schedule or on demand. As
new productions are announced or new data becomes available, the system
could automatically generate forecast reports and distribute them to
relevant stakeholders via email. A monthly report might summarise
predictions for all upcoming productions, flag significant changes from
previous forecasts, and highlight where additional data collection would
reduce uncertainty.\
Integration with CRM (Customer Relationship Management) and ticketing
systems would create feedback loops that improve predictions over time.
As actual sales data flows in for current productions, automated
processes could compare predictions to actuals, update performance
metrics, retrain models incorporating the new data, and adjust future
forecasts based on learned patterns. This continuous learning cycle
would gradually improve accuracy as more data accumulates.

## 11.3 Strategic Research Opportunities

Several research directions could extend the system\'s capabilities into
new domains or improve understanding of underlying demand dynamics.
Audience segmentation modelling would disaggregate predictions by patron
type. Rather than forecasting total tickets, separate models could
predict subscribers versus single-ticket buyers, tourists versus local
residents, or family groups versus individual adults. Understanding
which audience segments drive demand for different production types
would enable more targeted programming and marketing strategies.\
Causal inference methodologies could identify what factors genuinely
drive ticket sales rather than merely correlating with outcomes.
Techniques like difference-in-differences analysis, instrumental
variables, or causal forests could estimate the causal impact of
specific changes such as shifting opening dates, increasing marketing
spend, or partnering with community organisations. Causal understanding
would support more effective interventions to boost underperforming
productions or enhance successful ones.\
Price elasticity modelling could extend ticket volume predictions into
revenue forecasts by estimating how demand responds to price changes.
Training data might include periods with varying pricing strategies,
allowing estimation of how many additional tickets would sell at lower
prices or how much revenue would increase from premium pricing on
high-demand shows. These elasticity estimates would enable revenue
optimisation strategies that go beyond maximising attendance.

# Appendix A: Complete Mathematical Definitions

This appendix provides formal mathematical definitions for all feature
transformations and model equations referenced throughout the report.

## A.1 Target Variable Transformation

The target variable undergoes logarithmic transformation before
training: y = log(1 + max(mediantickets, 0)). The addition of one
handles zero values, whilst the max operation ensures non-negativity.
The inverse transformation recovers interpretable ticket counts:
ŷtickets = exp(ŷ) - 1.

Note: As of December 2024, the primary model operates on TicketIndex
(0-100 scale) rather than log-transformed tickets. The formula
TicketIndex ≈ 0.75 × SignalOnly + 27.3 produces index values that are
then scaled to ticket estimates via the benchmark: EstimatedTickets =
(TicketIndex/100) × 11,976.

## A.2 Ridge Regression Model

The constrained Ridge regression model minimises:

L = Σᵢ(yᵢ - wᵀxᵢ)² + α·\|\|w\|\|²

where:\
- yᵢ = TicketIndex for production i\
- xᵢ = feature vector (primarily SignalOnly score)\
- w = learned weight vector\
- α = 5.0 (regularisation parameter)\
- \|\|w\|\|² = sum of squared weights (L2 penalty)

The training dataset includes real productions plus synthetic anchor
points:\
- Anchor 1: (xanchor1, yanchor1) = (0, 25) representing low-buzz floor\
- Anchor 2: (xanchor2, yanchor2) = (100, 100) representing benchmark
alignment\
- Anchor weight: wanchor = max(3, nreal // 2)

The learned linear relationship is:

TicketIndex ≈ 0.75 × SignalOnly + 27.3

This formula provides the core predictive relationship, with the
intercept (27.3) representing the realistic floor for titles with no
measurable buzz signals, and the slope (0.75) indicating that SignalOnly
gains translate to proportionally smaller but still meaningful
TicketIndex improvements.\
Historical prior features aggregate past performance. The total prior
tickets sums all previous runs: xpriortotal = Σ ticketsi where datei \<
openingdate. The median prior captures typical performance:
xticketmedianprior = median({ticketsi : datei \< openingdate}). The
prior run count simply counts previous performances: xruncount = \|{i :
datei \< openingdate}\|.\
Remount timing features quantify how long since the last performance:
xyearssince = year(openingdate) - maxi year(datei). Boolean remount
indicators discretise this temporal gap: xisremountrecent =
𝟙{xyearssince ≤ 5} and xisremountmedium = 𝟙{2 \< xyearssince ≤ 5}.\
Date features extract temporal components. Month of opening ranges from
1 to 12: xmonth = month(openingdate). Week of year ranges from 1 to 52:
xweek = weekofyear(openingdate). Day of week encodes weekday versus
weekend: xdow = dayofweek(openingdate) ∈ {0, 1, ..., 6}. Holiday
indicators flag special periods: xholiday = 𝟙(openingdate ∈
HolidayWindows).\
Economic features normalise for inflation: xinflationadj =
CPI(referenceyear) / CPI(openingyear). This ratio converts historical
values to constant dollars. Other economic indicators like consumer
confidence, energy prices, and GDP growth are standardised to zero mean
and unit variance: xeconstandardised = (xeconraw - μ) / σ where μ and σ
are computed from the training set.\
Buzz features normalise platform-specific metrics to \[0, 1\]: xbuzz =
platformscore / max(historical_scores). This scaling ensures buzz
signals from different platforms contribute comparably despite different
absolute scales.

# Appendix B: Data Dictionary

Table 7: Key Features and Their Definitions

  -----------------------------------------------------------------------
  Feature                             Definition
  ----------------------------------- -----------------------------------
  WikiIdx                             Normalized Wikipedia popularity
                                      score

  TrendsIdx                           Google Trends normalized index

  YouTubeIdx                          YouTube presence score

  ChartmetricIdx                      Music artist familiarity score

  MusicMotivationBonus                Bonus based on high familiarity
                                      music
  -----------------------------------------------------------------------

# Appendix C: Glossary for Non-Technical Readers

Table 8: Plain-English Glossary

  -----------------------------------------------------------------------
  Term                                Meaning
  ----------------------------------- -----------------------------------
  Familiarity                         How well-known the show or title is

  Motivation                          How likely people are to buy based
                                      on excitement factors

  SignalOnly                          Sum of online indicators without
                                      any adjustment

  TicketIndex                         Normalized score for ticket
                                      projection

  Benchmark Title                     Title used to anchor ticket
                                      estimates (e.g., Cinderella)
  -----------------------------------------------------------------------

# Due Diligence Checklist

This checklist confirms that the Alberta Ballet Title Scoring App meets
quality standards for internal validation, deployment consideration, and
external review. Technical leads and engineering reviewers can use this
to verify system readiness.

## Data Quality & Leakage Control

All training features available at forecast time: Confirmed. The
getcvsplitter() function enforces time-aware cross-validation, and
feature engineering modules respect temporal boundaries by design.
Automated tests in testnoleakageindataset.py verify this property
systematically.\
No future-target leakage present: Confirmed. The TimeSeriesCVSplitter in
ml/timesplits.py ensures validation data always comes after training
data chronologically. The leakage audit file flags problematic features
for exclusion.\
Data validation scripts exist and run: Confirmed. Scripts including
runfullpipeline.py, testdataquality.py, and testleakage.py validate data
integrity and temporal consistency.\
Prior ticketing data normalised and lagged: Confirmed. Feature
computation in titlefeatures.py and utils/priors.py applies appropriate
temporal filtering and inflation adjustment.\
Economic and buzz features aligned with openingdate: Confirmed. Features
are computed using only data available before each production\'s opening
date, implemented in economicfeatures.py and buzzfeatures.py.

## Modelling Workflow

Model training reproducible via CLI: Confirmed. The command python
scripts/trainsafemodel.py with appropriate flags executes the complete
training pipeline.\
Hyperparameter tuning implemented: Confirmed. The \--tune flag enables
systematic optimisation through cross-validation.\
Model artefacts saved with metadata: Confirmed. Joblib and JSON files in
outputs/models/ preserve complete model state and configuration.\
Cross-validation respects time: Confirmed. Five-fold
TimeSeriesCVSplitter ensures chronological ordering.\
SHAP explainability available: Confirmed. The \--save-shap flag triggers
SHAP value computation and storage.

## Testing and Validation

Automated tests for leakage detection: Confirmed. Test suite includes
temporal validation checks that verify no future information enters
training data.\
Performance metrics documented: Confirmed. Cross-validation results
including MAE, RMSE, and R² are computed and stored with each training
run.\
Error analysis by segment available: Confirmed. Diagnostic scripts
analyse prediction errors by production category, city, and remount
status.

--- End of Document ---

### 🎯 Benchmark Title Justification

The model uses a benchmark production to convert normalized TicketIndex
scores into real-world ticket estimates. In this case, the chosen
benchmark is Cinderella, which had a de-seasonalized median of
approximately 12,000 tickets.

This selection is intentional:\
- Cinderella represents the highest-performing production in our
history, aside from The Nutcracker, which is considered a structural
outlier due to its annual repetition, holiday timing, and deep cultural
resonance.\
- Using Cinderella as the benchmark avoids the distortion of modeling
every show as a "holiday blockbuster," while still anchoring the model
to a production with broad appeal, familiarity, and production value.\
- This provides a realistic upper-bound reference for main-stage
narrative ballets, while not artificially inflating ticket estimates
across the board.

While Giselle or a smaller mixed-bill production could offer a more
"average" benchmark, this would:\
- Compress the score range between hits and underperformers\
- Under-represent upside potential for shows with high cultural buzz or
narrative weight\
- Risk creating overly conservative estimates in the upper tier

Therefore, Cinderella was chosen as the most suitable "non-outlier
high-performer", balancing realism and aspiration.
